{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd24f33b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\ankush.parkhad\\anaconda3\\lib\\site-packages (4.7.2)\n",
      "Requirement already satisfied: urllib3[socks]~=1.26 in c:\\users\\ankush.parkhad\\anaconda3\\lib\\site-packages (from selenium) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\ankush.parkhad\\anaconda3\\lib\\site-packages (from selenium) (2021.10.8)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\ankush.parkhad\\anaconda3\\lib\\site-packages (from selenium) (0.22.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\ankush.parkhad\\anaconda3\\lib\\site-packages (from selenium) (0.9.2)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\ankush.parkhad\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: outcome in c:\\users\\ankush.parkhad\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\ankush.parkhad\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\ankush.parkhad\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (21.4.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\ankush.parkhad\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.0)\n",
      "Requirement already satisfied: idna in c:\\users\\ankush.parkhad\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.3)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in c:\\users\\ankush.parkhad\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.0.4)\n",
      "Requirement already satisfied: async-generator>=1.9 in c:\\users\\ankush.parkhad\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.10)\n",
      "Requirement already satisfied: pycparser in c:\\users\\ankush.parkhad\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\ankush.parkhad\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\ankush.parkhad\\anaconda3\\lib\\site-packages (from urllib3[socks]~=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\ankush.parkhad\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c6244b",
   "metadata": {},
   "source": [
    "# Q1: Write a python program to scrape data for “Data Analyst” Job position in “Bangalore” location. You have to scrape the job-title, job-location, company_name, experience_required. You have to scrape first 10 jobs data.\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.naukri.com/\n",
    "2. Enter “Data Analyst” in “Skill, Designations, Companies” field and enter “Bangalore” in “enter the\n",
    "location” field.\n",
    "3. Then click the search button.\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19beda77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "# Now we will download the webdriver for the Web Browser. Steps for downloading are:\n",
    "#1. Check the version of your browser.\n",
    "#2. go to the link  https://chromedriver.chromium.org/downloads\n",
    "#3. Download the webdriver for your version of your browser.\n",
    "\n",
    "#First connecting to the web driver\n",
    "driver=webdriver.Chrome(r'C:\\Users\\ankush.parkhad\\Desktop\\data science\\chrome driver\\chromedriver.exe')\n",
    "\n",
    "#opening the naukri page on automated chrome browser\n",
    "driver.get('https://www.naukri.com/')\n",
    "\n",
    "#entering designation and location as required in the question\n",
    "designation=driver.find_element(By.CLASS_NAME,'suggestor-input ')\n",
    "designation.send_keys('Data Analyst')\n",
    "\n",
    "location=driver.find_element(By.XPATH,'/html/body/div[1]/div[6]/div/div/div[5]/div/div/div/input') #here we have to extract only one element thats why we are using absolute xpath here\n",
    "location.send_keys('Bangalore')\n",
    "\n",
    "search=driver.find_element(By.CLASS_NAME,'qsbSubmit')\n",
    "search.click()\n",
    "\n",
    "#creating the empty lists\n",
    "job_title=[]\n",
    "job_location=[]\n",
    "company_name=[]\n",
    "experience_required=[]\n",
    "\n",
    "#scrapping job titles from the given page\n",
    "title_tags=driver.find_elements(By.XPATH,'//a[@class=\"title fw500 ellipsis\"]')#here i have to extract more than one path thats why we are using relative xpath here\n",
    "for i in title_tags[0:10]:\n",
    "    title=i.text\n",
    "    job_title.append(title)\n",
    "\n",
    "#scrapping job location from the given page\n",
    "location_tags=driver.find_elements(By.XPATH,'//span[@class=\"ellipsis fleft fs12 lh16 locWdth\"]')\n",
    "for i in location_tags[0:10]:\n",
    "    locations=i.text\n",
    "    job_location.append(locations)\n",
    "\n",
    "#scrapping company name from the given page\n",
    "company_tags=driver.find_elements(By.XPATH,'//a[@class=\"subTitle ellipsis fleft\"]')\n",
    "for i in company_tags[0:10]:\n",
    "    company=i.text\n",
    "    company_name.append(company)\n",
    "\n",
    "#scrapping job experience from the given page\n",
    "experience_tags=driver.find_elements(By.XPATH,'//span[@class=\"ellipsis fleft fs12 lh16 expwdth\"]')\n",
    "for i in experience_tags[0:10]:\n",
    "    exp=i.text\n",
    "    experience_required.append(exp)\n",
    "\n",
    "print(len(job_title), len(job_location), len(company_name), len(experience_required))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "125d9356",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job Title</th>\n",
       "      <th>Job Location</th>\n",
       "      <th>Company Name</th>\n",
       "      <th>Experience Required</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Shell Pvt Ltd</td>\n",
       "      <td>2-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Shell Pvt Ltd</td>\n",
       "      <td>3-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Shell Pvt Ltd</td>\n",
       "      <td>2-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Senior Data Analyst II</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Flipkart</td>\n",
       "      <td>2-4 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Project Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>WSP</td>\n",
       "      <td>0-4 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data Analyst, Business Analyst</td>\n",
       "      <td>Bangalore/Bengaluru(HSR Layout +1)</td>\n",
       "      <td>Clinilaunch Research Institute Llp</td>\n",
       "      <td>0-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Temp. WFH - Bangalore/Bengaluru</td>\n",
       "      <td>Treebo Hotels</td>\n",
       "      <td>1-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Executive - Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Flipkart</td>\n",
       "      <td>1-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Cargill</td>\n",
       "      <td>2-4 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Eli Lilly And Company</td>\n",
       "      <td>2-6 Yrs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Job Title                        Job Location  \\\n",
       "0                    Data Analyst                 Bangalore/Bengaluru   \n",
       "1                    Data Analyst                 Bangalore/Bengaluru   \n",
       "2                    Data Analyst                 Bangalore/Bengaluru   \n",
       "3          Senior Data Analyst II                 Bangalore/Bengaluru   \n",
       "4            Project Data Analyst                 Bangalore/Bengaluru   \n",
       "5  Data Analyst, Business Analyst  Bangalore/Bengaluru(HSR Layout +1)   \n",
       "6                    Data Analyst     Temp. WFH - Bangalore/Bengaluru   \n",
       "7        Executive - Data Analyst                 Bangalore/Bengaluru   \n",
       "8                    Data Analyst                 Bangalore/Bengaluru   \n",
       "9                    Data Analyst                 Bangalore/Bengaluru   \n",
       "\n",
       "                         Company Name Experience Required  \n",
       "0                       Shell Pvt Ltd             2-5 Yrs  \n",
       "1                       Shell Pvt Ltd             3-5 Yrs  \n",
       "2                       Shell Pvt Ltd             2-5 Yrs  \n",
       "3                            Flipkart             2-4 Yrs  \n",
       "4                                 WSP             0-4 Yrs  \n",
       "5  Clinilaunch Research Institute Llp             0-5 Yrs  \n",
       "6                       Treebo Hotels             1-5 Yrs  \n",
       "7                            Flipkart             1-5 Yrs  \n",
       "8                             Cargill             2-4 Yrs  \n",
       "9               Eli Lilly And Company             2-6 Yrs  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating the dataframe from the above data\n",
    "df=pd.DataFrame({'Job Title': job_title, 'Job Location':job_location,'Company Name':company_name, 'Experience Required':experience_required })\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524bd205",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb1d632",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "acfdbcd4",
   "metadata": {},
   "source": [
    "# Q2:Write a python program to scrape data for “Data Scientist” Job position in “Bangalore” location. \n",
    "You have to scrape the job-title, job-location, company_name. You have to scrape first 10 jobs data.\n",
    "\n",
    "This task will be done in following steps:\n",
    "\n",
    "1. First get the webpage https://www.naukri.com/\n",
    "\n",
    "2. Enter “Data Scientist” in “Skill, Designations, Companies” field and enter “Bangalore” in “enter the\n",
    "location” field.\n",
    "\n",
    "3. Then click the search button.\n",
    "\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "\n",
    "5. Finally create a dataframe of the scraped data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdae7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first importing the required libraries\n",
    "import pandas as pd\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "\n",
    "\n",
    " #Now we will download the webdriver for the Web Browser. Steps for downloading are:\n",
    "#1. Check the version of your browser.\n",
    "#2. go to the link  https://chromedriver.chromium.org/downloads\n",
    "#3. Download the webdriver for your version of your browser.\n",
    "\n",
    "#first connecting to the web driver\n",
    "driver=webdriver.Chrome(r'C:\\Users\\ankush.parkhad\\Desktop\\data science\\chrome driver\\chromedriver.exe')\n",
    "\n",
    "#opening the naukri page on automated chrome broswser\n",
    "driver.get('https://www.naukri.com/')\n",
    "\n",
    "#entering the designation and location as required in the quesion\n",
    "designation=driver.find_element(By.CLASS_NAME,'suggestor-input ')\n",
    "designation.send_keys('Data Scientist')\n",
    "\n",
    "location=driver.find_element(By.XPATH,'/html/body/div[1]/div[6]/div/div/div[5]/div/div/div/input')\n",
    "location.send_keys('Bangalore')\n",
    "\n",
    "#clicking the search button\n",
    "search=driver.find_element(By.CLASS_NAME,'qsbSubmit')\n",
    "search.click()\n",
    "\n",
    "#creating the empty lists\n",
    "job_title=[]\n",
    "job_location=[]\n",
    "company_name=[]\n",
    "\n",
    "#scrapping job title from the given page\n",
    "title_tags=driver.find_elements(By.XPATH,'//a[@class=\"title fw500 ellipsis\"]')#here i have to extract more than one path thats why we are using relative xpath here\n",
    "for i in title_tags[0:10]:\n",
    "    title=i.text\n",
    "    job_title.append(title)\n",
    "    \n",
    "#scrapping job location from the given page\n",
    "location_tags=driver.find_elements(By.XPATH,'//span[@class=\"ellipsis fleft fs12 lh16 locWdth\"]')\n",
    "for i in location_tags[0:10]:\n",
    "    location=i.text\n",
    "    job_location.append(location)\n",
    "\n",
    "    \n",
    "#scrapping company name from the given page\n",
    "company_tags=driver.find_elements(By.XPATH,'//a[@class=\"subTitle ellipsis fleft\"]')\n",
    "for i in company_tags[0:10]:\n",
    "    company=i.text\n",
    "    company_name.append(company)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c14f43c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job Title</th>\n",
       "      <th>Job Location</th>\n",
       "      <th>Company Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Analystics &amp; Modeling Specialist</td>\n",
       "      <td>Bangalore/Bengaluru, Kolkata, Mumbai, Hyderaba...</td>\n",
       "      <td>Accenture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru, New Delhi, Hyderabad/Secu...</td>\n",
       "      <td>Tata Nexarc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru, Nagpur, Pune</td>\n",
       "      <td>Tech Mahindra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data &amp; Analytics Lead, Geo Analytics - GAMMA</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Boston Consulting Group</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Weather and Climate Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Shell Pvt Ltd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Hybrid - Bangalore/Bengaluru, Noida, Kolkata, ...</td>\n",
       "      <td>Mindtree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data Scientist - II</td>\n",
       "      <td>Bangalore/Bengaluru, India, Mumbai (All Areas)</td>\n",
       "      <td>Bizongo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru, Mumbai</td>\n",
       "      <td>Baker Hughes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Manager-Data Science</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>AMERICAN EXPRESS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ACN - Applied Intelligence - Data Scientist - 09</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Accenture</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Job Title  \\\n",
       "0                  Analystics & Modeling Specialist   \n",
       "1                                    Data Scientist   \n",
       "2                                    Data Scientist   \n",
       "3      Data & Analytics Lead, Geo Analytics - GAMMA   \n",
       "4                Weather and Climate Data Scientist   \n",
       "5                                    Data Scientist   \n",
       "6                               Data Scientist - II   \n",
       "7                             Senior Data Scientist   \n",
       "8                              Manager-Data Science   \n",
       "9  ACN - Applied Intelligence - Data Scientist - 09   \n",
       "\n",
       "                                        Job Location             Company Name  \n",
       "0  Bangalore/Bengaluru, Kolkata, Mumbai, Hyderaba...                Accenture  \n",
       "1  Bangalore/Bengaluru, New Delhi, Hyderabad/Secu...              Tata Nexarc  \n",
       "2                  Bangalore/Bengaluru, Nagpur, Pune            Tech Mahindra  \n",
       "3                                Bangalore/Bengaluru  Boston Consulting Group  \n",
       "4                                Bangalore/Bengaluru            Shell Pvt Ltd  \n",
       "5  Hybrid - Bangalore/Bengaluru, Noida, Kolkata, ...                 Mindtree  \n",
       "6     Bangalore/Bengaluru, India, Mumbai (All Areas)                  Bizongo  \n",
       "7                        Bangalore/Bengaluru, Mumbai             Baker Hughes  \n",
       "8                                Bangalore/Bengaluru         AMERICAN EXPRESS  \n",
       "9                                Bangalore/Bengaluru                Accenture  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating the dataframe from the above data\n",
    "df=pd.DataFrame({'Job Title': job_title, 'Job Location':job_location,'Company Name':company_name})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6ddbc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eca09da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aec4faf4",
   "metadata": {},
   "source": [
    "# Q3: In this question you have to scrape data using the filters available on the webpage as shown below:\n",
    "You have to use the location and salary filter.\n",
    "\n",
    "You have to scrape data for “Data Scientist” designation for first 10 job results.\n",
    "\n",
    "You have to scrape the job-title, job-location, company name, experience required.\n",
    "\n",
    "The location filter to be used is “Delhi/NCR”. The salary filter to be used is “3-6” lakhs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "26cff6c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job Title</th>\n",
       "      <th>Job Location</th>\n",
       "      <th>Company Name</th>\n",
       "      <th>Experience Required</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Artificial Intelligence/Computer Vision Engine...</td>\n",
       "      <td>Delhi / NCR, Kolkata, Mumbai, Hyderabad/Secund...</td>\n",
       "      <td>Vicara</td>\n",
       "      <td>1-3 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Activation Specialist - Adobe Target</td>\n",
       "      <td>Delhi / NCR, Kolkata, Mumbai, Hyderabad/Secund...</td>\n",
       "      <td>Okda Solutions</td>\n",
       "      <td>7-10 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Scientist - Engine Algorithm</td>\n",
       "      <td>Delhi / NCR, Kolkata, Mumbai, Hyderabad/Secund...</td>\n",
       "      <td>Primo Hiring</td>\n",
       "      <td>1-3 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Temp. WFH - Noida</td>\n",
       "      <td>NGI Ventures</td>\n",
       "      <td>1-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Opening For Jr. Data Scientist with Tatras Dat...</td>\n",
       "      <td>Delhi / NCR</td>\n",
       "      <td>Tatras Data Services</td>\n",
       "      <td>2-4 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Dehradun, Hyderabad/Secunderabad, Gurgaon/Guru...</td>\n",
       "      <td>torcai digital media</td>\n",
       "      <td>2-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Gurgaon/Gurugram</td>\n",
       "      <td>Americana Restaurants (india)</td>\n",
       "      <td>3-8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>URGENT: Data Scientist | Gurugram | 5 Days Wor...</td>\n",
       "      <td>Gurgaon/Gurugram</td>\n",
       "      <td>Digilytics</td>\n",
       "      <td>2-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Noida</td>\n",
       "      <td>Alliance Recruitment Agency</td>\n",
       "      <td>3-4 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>SE/SSE-Data Scientist</td>\n",
       "      <td>Delhi / NCR</td>\n",
       "      <td>Bold Technology Systems</td>\n",
       "      <td>3-8 Yrs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Job Title  \\\n",
       "0  Artificial Intelligence/Computer Vision Engine...   \n",
       "1          Data Activation Specialist - Adobe Target   \n",
       "2                  Data Scientist - Engine Algorithm   \n",
       "3                                     Data Scientist   \n",
       "4  Opening For Jr. Data Scientist with Tatras Dat...   \n",
       "5                                     Data Scientist   \n",
       "6                                     Data Scientist   \n",
       "7  URGENT: Data Scientist | Gurugram | 5 Days Wor...   \n",
       "8                                     Data Scientist   \n",
       "9                              SE/SSE-Data Scientist   \n",
       "\n",
       "                                        Job Location  \\\n",
       "0  Delhi / NCR, Kolkata, Mumbai, Hyderabad/Secund...   \n",
       "1  Delhi / NCR, Kolkata, Mumbai, Hyderabad/Secund...   \n",
       "2  Delhi / NCR, Kolkata, Mumbai, Hyderabad/Secund...   \n",
       "3                                  Temp. WFH - Noida   \n",
       "4                                        Delhi / NCR   \n",
       "5  Dehradun, Hyderabad/Secunderabad, Gurgaon/Guru...   \n",
       "6                                   Gurgaon/Gurugram   \n",
       "7                                   Gurgaon/Gurugram   \n",
       "8                                              Noida   \n",
       "9                                        Delhi / NCR   \n",
       "\n",
       "                    Company Name Experience Required  \n",
       "0                         Vicara             1-3 Yrs  \n",
       "1                 Okda Solutions            7-10 Yrs  \n",
       "2                   Primo Hiring             1-3 Yrs  \n",
       "3                   NGI Ventures             1-5 Yrs  \n",
       "4           Tatras Data Services             2-4 Yrs  \n",
       "5           torcai digital media             2-7 Yrs  \n",
       "6  Americana Restaurants (india)             3-8 Yrs  \n",
       "7                     Digilytics             2-5 Yrs  \n",
       "8    Alliance Recruitment Agency             3-4 Yrs  \n",
       "9        Bold Technology Systems             3-8 Yrs  "
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#first importing the required libraries\n",
    "import pandas as pd\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "\n",
    "\n",
    " #Now we will download the webdriver for the Web Browser. Steps for downloading are:\n",
    "#1. Check the version of your browser.\n",
    "#2. go to the link  https://chromedriver.chromium.org/downloads\n",
    "#3. Download the webdriver for your version of your browser.\n",
    "\n",
    "#first connecting to the web driver\n",
    "driver=webdriver.Chrome(r'C:\\Users\\ankush.parkhad\\Desktop\\data science\\chrome driver\\chromedriver.exe')\n",
    "\n",
    "#opening the naukri page on automated chrome broswser\n",
    "driver.get('https://www.naukri.com/')\n",
    "\n",
    "#entering the designation as required in the quesion\n",
    "designation=driver.find_element(By.CLASS_NAME,'suggestor-input ')\n",
    "designation.send_keys('Data Scientist')\n",
    "\n",
    "#clicking the search button\n",
    "search=driver.find_element(By.CLASS_NAME,'qsbSubmit')\n",
    "search.click()\n",
    "\n",
    "#ticking the location(Delhi/NCR) box\n",
    "location=driver.find_element(By.XPATH,'/html/body/div[1]/div[4]/div/div/section[1]/div[2]/div[5]/div[2]/div[2]/label/p/span[1]')\n",
    "location.click()\n",
    "\n",
    "\n",
    "#ticking the salary range(3-6 lakhs)\n",
    "salary=driver.find_element(By.XPATH,'/html/body/div[1]/div[4]/div/div/section[1]/div[2]/div[6]/div[2]/div[2]/label/p/span[1]')\n",
    "salary.click()\n",
    "\n",
    "#creating empty lists\n",
    "title=[]\n",
    "location=[]\n",
    "company=[]\n",
    "exp=[]\n",
    "\n",
    "#scrapping first 10 job titles\n",
    "job=driver.find_elements(By.XPATH,'//a[@class=\"title fw500 ellipsis\"]')\n",
    "for i in job[0:10]:\n",
    "    title.append(i.text)\n",
    "    \n",
    "#scrapping first 10 job location\n",
    "loc=driver.find_elements(By.XPATH,'//span[@class=\"ellipsis fleft fs12 lh16 locWdth\"]')\n",
    "for i in loc[0:10]:\n",
    "    location.append(i.text)\n",
    "    \n",
    "#scrapping first 10 job company\n",
    "com=driver.find_elements(By.XPATH,'//a[@class=\"subTitle ellipsis fleft\"]')\n",
    "for i in com[0:10]:\n",
    "    company.append(i.text)\n",
    "    \n",
    "#scrapping first 10 job experience\n",
    "experience=driver.find_elements(By.XPATH,'//span[@class=\"ellipsis fleft fs12 lh16 expwdth\"]')\n",
    "for i in experience[0:10]:\n",
    "    exp.append(i.text)\n",
    "    \n",
    "#creating dataframe\n",
    "df=pd.DataFrame({'Job Title':title, 'Job Location':location, 'Company Name':company, 'Experience Required':exp})\n",
    "df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d90b4da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2002e1af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8041f077",
   "metadata": {},
   "source": [
    "# Q4: Scrape data of first 100 sunglasses listings on flipkart.com. You have to scrape four attributes:\n",
    "1. Brand\n",
    "\n",
    "2. ProductDescription\n",
    "\n",
    "3. Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "9ab7fc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first importing the required libraries\n",
    "import pandas as pd\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "\n",
    "\n",
    " #Now we will download the webdriver for the Web Browser. Steps for downloading are:\n",
    "#1. Check the version of your browser.\n",
    "#2. go to the link  https://chromedriver.chromium.org/downloads\n",
    "#3. Download the webdriver for your version of your browser.\n",
    "\n",
    "#first connecting to the web driver\n",
    "driver=webdriver.Chrome(r'C:\\Users\\ankush.parkhad\\Desktop\\data science\\chrome driver\\chromedriver.exe')\n",
    "\n",
    "#opening the flipkart page on automated chrome broswser\n",
    "driver.get('https://www.flipkart.com/')\n",
    "\n",
    "#typing sunglasses in search box\n",
    "product=driver.find_element(By.CLASS_NAME,'_3704LK')\n",
    "product.send_keys('Sunglasses')\n",
    "\n",
    "\n",
    "#clicking the search button\n",
    "search=driver.find_element(By.CLASS_NAME,'L0Z3Pu')\n",
    "search.click()\n",
    "\n",
    "#creating empty lists\n",
    "brand=[]\n",
    "desc=[]\n",
    "price=[]\n",
    "discount=[]\n",
    "\n",
    "#scrapping brand for 100 sunglasses\n",
    "start=0\n",
    "end=3\n",
    "for page in range(start,end):\n",
    "    sunglasses=driver.find_elements(By.XPATH,'//div[@class=\"_2WkVRV\"]')\n",
    "    for i in sunglasses[0:100]:\n",
    "        brand.append(i.text)\n",
    "\n",
    "#scrapping desc for 100 sunglasses\n",
    "start=0\n",
    "end=3\n",
    "for page in range(start,end):\n",
    "    des=driver.find_elements(By.XPATH,'//a[@class=\"IRpwTa\"]')\n",
    "    for i in des[0:100]:\n",
    "        desc.append(i.text)\n",
    "\n",
    "#scrapping price for 100 sunglasses\n",
    "start=0\n",
    "end=3\n",
    "for page in range(start,end):\n",
    "    pr=driver.find_elements(By.XPATH,'//div[@class=\"_30jeq3\"]')\n",
    "    for i in pr[0:100]:\n",
    "        price.append(i.text)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "45df4e8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand</th>\n",
       "      <th>Discription</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ROZZETTA CRAFT</td>\n",
       "      <td>UV Protection, Gradient Retro Square Sunglasse...</td>\n",
       "      <td>₹549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Silver Kartz</td>\n",
       "      <td>UV Protection Clubmaster Sunglasses (53)</td>\n",
       "      <td>₹278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LIZA ANGEL</td>\n",
       "      <td>Riding Glasses, Night Vision Spectacle Sunglas...</td>\n",
       "      <td>₹129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fastrack</td>\n",
       "      <td>UV Protection Rectangular Sunglasses (Free Size)</td>\n",
       "      <td>₹489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Elligator</td>\n",
       "      <td>UV Protection Cat-eye, Retro Square, Oval, Rou...</td>\n",
       "      <td>₹149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>VINCENT CHASE</td>\n",
       "      <td>by Lenskart Polarized, UV Protection Rectangul...</td>\n",
       "      <td>₹719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Elligator</td>\n",
       "      <td>UV Protection Round Sunglasses (54)</td>\n",
       "      <td>₹286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>New Specs</td>\n",
       "      <td>UV Protection Rectangular Sunglasses (Free Size)</td>\n",
       "      <td>₹259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>LIZA ANGEL</td>\n",
       "      <td>UV Protection, Polarized Rectangular Sunglasse...</td>\n",
       "      <td>₹129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>VINCENT CHASE</td>\n",
       "      <td>by Lenskart Polarized, UV Protection Cat-eye S...</td>\n",
       "      <td>₹949</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Brand                                        Discription Price\n",
       "0   ROZZETTA CRAFT  UV Protection, Gradient Retro Square Sunglasse...  ₹549\n",
       "1     Silver Kartz           UV Protection Clubmaster Sunglasses (53)  ₹278\n",
       "2       LIZA ANGEL  Riding Glasses, Night Vision Spectacle Sunglas...  ₹129\n",
       "3         Fastrack   UV Protection Rectangular Sunglasses (Free Size)  ₹489\n",
       "4        Elligator  UV Protection Cat-eye, Retro Square, Oval, Rou...  ₹149\n",
       "..             ...                                                ...   ...\n",
       "95   VINCENT CHASE  by Lenskart Polarized, UV Protection Rectangul...  ₹719\n",
       "96       Elligator                UV Protection Round Sunglasses (54)  ₹286\n",
       "97       New Specs   UV Protection Rectangular Sunglasses (Free Size)  ₹259\n",
       "98      LIZA ANGEL  UV Protection, Polarized Rectangular Sunglasse...  ₹129\n",
       "99   VINCENT CHASE  by Lenskart Polarized, UV Protection Cat-eye S...  ₹949\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating dataframe\n",
    "df=pd.DataFrame({'Brand':brand, 'Discription':desc, 'Price':price})\n",
    "df[0:100]\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415591f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a132082d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e5d69d09",
   "metadata": {},
   "source": [
    "# Q5: Scrape 100 reviews data from flipkart.com for iphone11 phone. You have to go the link:\n",
    "https://www.flipkart.com/apple-iphone-11-black-64-gb/product\u0002reviews/itm4e5041ba101fd?pid=MOBFWQ6BXGJCEYNY&lid=LSTMOBFWQ6BXGJCEYNYZXSHRJ&market\n",
    "place=FLIPKART\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8507d4c",
   "metadata": {},
   "source": [
    "As the link given in the ques is saying \"Unfortunately the page you are looking for has been moved or deleted\", so i searched the same product from flipkart by myself and completing the question from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "e5db0134",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first importing the required libraries\n",
    "import pandas as pd\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "\n",
    "\n",
    " #Now we will download the webdriver for the Web Browser. Steps for downloading are:\n",
    "#1. Check the version of your browser.\n",
    "#2. go to the link  https://chromedriver.chromium.org/downloads\n",
    "#3. Download the webdriver for your version of your browser.\n",
    "\n",
    "#first connecting to the web driver\n",
    "driver=webdriver.Chrome(r'C:\\Users\\ankush.parkhad\\Desktop\\data science\\chrome driver\\chromedriver.exe')\n",
    "\n",
    "#opening the flipkart page on automated chrome broswser\n",
    "driver.get('https://www.flipkart.com/apple-iphone-11-black-64-gb/product-reviews/itm4e5041ba101fd?pid=MOBFWQ6BXGJCEYNY&lid=LSTMOBFWQ6BXGJCEYNYZXSHRJ&marketplace=FLIPKART')\n",
    "\n",
    "#creating empty lists\n",
    "rating=[]\n",
    "review_summary=[]\n",
    "full_review=[]\n",
    "\n",
    "\n",
    "#scrapping first 100 ratings\n",
    "start=0\n",
    "end=10\n",
    "for page in range(start,end):\n",
    "    rate=driver.find_elements(By.XPATH,'//div[@class=\"_3LWZlK _1BLPMq\"]')\n",
    "    for i in rate:\n",
    "        rating.append(i.text)\n",
    "\n",
    "#scrapping first 100 review summary\n",
    "start=0\n",
    "end=10\n",
    "for page in range(start,end):\n",
    "    summary=driver.find_elements(By.XPATH,'//p[@class=\"_2-N8zT\"]')\n",
    "    for i in summary:\n",
    "        review_summary.append(i.text)\n",
    "\n",
    "#scrapping first 100 full review\n",
    "start=0\n",
    "end=10\n",
    "for page in range(start,end):\n",
    "    full=driver.find_elements(By.XPATH,'//div[@class=\"t-ZTKy\"]')\n",
    "    for i in full:\n",
    "        full_review.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "5b8718cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ratings</th>\n",
       "      <th>Review_summary</th>\n",
       "      <th>Full Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Simply awesome</td>\n",
       "      <td>Really satisfied with the Product I received.....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>Perfect product!</td>\n",
       "      <td>Amazing phone with great cameras and better ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Best in the market!</td>\n",
       "      <td>Great iPhone very snappy experience as apple k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Value-for-money</td>\n",
       "      <td>I'm Really happy with the product\\nDelivery wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Worth every penny</td>\n",
       "      <td>Previously I was using one plus 3t it was a gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>5</td>\n",
       "      <td>Highly recommended</td>\n",
       "      <td>It's my first time to use iOS phone and I am l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>5</td>\n",
       "      <td>Highly recommended</td>\n",
       "      <td>What a camera .....just awesome ..you can feel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>5</td>\n",
       "      <td>Perfect product!</td>\n",
       "      <td>Value for money\\n5 star rating\\nExcellent came...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>5</td>\n",
       "      <td>Great product</td>\n",
       "      <td>Amazing Powerful and Durable Gadget.\\n\\nI’m am...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>4</td>\n",
       "      <td>Pretty good</td>\n",
       "      <td>I was using Iphone 6s and also Oneplus 6t. Bot...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Ratings       Review_summary  \\\n",
       "0        5       Simply awesome   \n",
       "1        5     Perfect product!   \n",
       "2        5  Best in the market!   \n",
       "3        4      Value-for-money   \n",
       "4        5    Worth every penny   \n",
       "..     ...                  ...   \n",
       "95       5   Highly recommended   \n",
       "96       5   Highly recommended   \n",
       "97       5     Perfect product!   \n",
       "98       5        Great product   \n",
       "99       4          Pretty good   \n",
       "\n",
       "                                          Full Review  \n",
       "0   Really satisfied with the Product I received.....  \n",
       "1   Amazing phone with great cameras and better ba...  \n",
       "2   Great iPhone very snappy experience as apple k...  \n",
       "3   I'm Really happy with the product\\nDelivery wa...  \n",
       "4   Previously I was using one plus 3t it was a gr...  \n",
       "..                                                ...  \n",
       "95  It's my first time to use iOS phone and I am l...  \n",
       "96  What a camera .....just awesome ..you can feel...  \n",
       "97  Value for money\\n5 star rating\\nExcellent came...  \n",
       "98  Amazing Powerful and Durable Gadget.\\n\\nI’m am...  \n",
       "99  I was using Iphone 6s and also Oneplus 6t. Bot...  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating dataframe\n",
    "df=pd.DataFrame({'Ratings':rating, 'Review_summary':review_summary, 'Full Review':full_review})\n",
    "df\n",
    "  "
   ]
  },
  {
   "cell_type": "raw",
   "id": "38a421ef",
   "metadata": {},
   "source": [
    "Below done question is throwing error at every point and i raised so much of tickets still the code didnt run and continously throwing errors thats why i left them in the middle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ea2ff9",
   "metadata": {},
   "source": [
    "# Q6: Scrape data for first 100 sneakers you find when you visit flipkart.com and search for “sneakers” in the search field.\n",
    "You have to scrape 4 attributes of each sneaker:\n",
    "\n",
    "1. Brand\n",
    "\n",
    "2. ProductDescription\n",
    "\n",
    "3. Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6b95fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first importing the required libraries\n",
    "import pandas as pd\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "\n",
    "\n",
    " #Now we will download the webdriver for the Web Browser. Steps for downloading are:\n",
    "#1. Check the version of your browser.\n",
    "#2. go to the link  https://chromedriver.chromium.org/downloads\n",
    "#3. Download the webdriver for your version of your browser.\n",
    "\n",
    "#first connecting to the web driver\n",
    "driver=webdriver.Chrome(r'C:\\Users\\ankush.parkhad\\Desktop\\data science\\chrome driver\\chromedriver.exe')\n",
    "\n",
    "#opening the flipkart page on automated chrome broswser\n",
    "driver.get('https://www.flipkart.com/')\n",
    "\n",
    "#searching sneakers in search field\n",
    "product=driver.find_element(By.CLASS_NAME,'_3704LK')\n",
    "product.send_keys('Sneakers')\n",
    "\n",
    "#clickin search button\n",
    "search=driver.find_element(By.CLASS_NAME,'L0Z3Pu')\n",
    "search.click()\n",
    "\n",
    "#creating empty lists\n",
    "brand=[]\n",
    "prod_desc=[]\n",
    "price=[]\n",
    "\n",
    "#getting brand of first 100 sneakers\n",
    "start=0\n",
    "end=3\n",
    "for page in range(start,end):\n",
    "    brands=driver.find_element(By.XPATH,'//div[@class=\"_2WkVRV\"]')\n",
    "    for i in brands[0:100]:\n",
    "        brand.append(i)\n",
    "    next_button=driver.find_element(By.XPATH,'//a[@class=\"_1LKTO3\"]')\n",
    "    next_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33852e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "70cedede",
   "metadata": {},
   "source": [
    "# Q7: Go to webpage https://www.amazon.in/\n",
    "Enter “Laptop” in the search field and then click the search icon.\n",
    "Then set CPU Type filter to “Intel Core i7” as shown in the below image:\n",
    "    \n",
    "After setting the filters scrape first 10 laptops data. You have to scrape 3 attributes for each laptop:\n",
    "    \n",
    "1. Title\n",
    "\n",
    "2. Ratings\n",
    "\n",
    "3. Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c00c980",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first importing the required libraries\n",
    "import pandas as pd\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "\n",
    "\n",
    " #Now we will download the webdriver for the Web Browser. Steps for downloading are:\n",
    "#1. Check the version of your browser.\n",
    "#2. go to the link  https://chromedriver.chromium.org/downloads\n",
    "#3. Download the webdriver for your version of your browser.\n",
    "\n",
    "#first connecting to the web driver\n",
    "driver=webdriver.Chrome(r'C:\\Users\\ankush.parkhad\\Desktop\\data science\\chrome driver\\chromedriver.exe')\n",
    "\n",
    "#opening the flipkart page on automated chrome broswser\n",
    "driver.get(' https://www.amazon.in/')\n",
    "\n",
    "\n",
    "#entering sneakers in the search field\n",
    "product=driver.find_element(By.XPATH,'/html/body/div[1]/header/div/div[1]/div[2]/div/form/div[2]/div[1]')\n",
    "product.send_keys('Sneakers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2b15c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4db5d0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "57157ee2",
   "metadata": {},
   "source": [
    "# Q8: Write a python program to scrape data for Top 1000 Quotes of All Time.The above task will be done in following steps:\n",
    "1.First get the webpage https://www.azquotes.com/\n",
    "\n",
    "2. Click on Top Quotes\n",
    "\n",
    "\n",
    "3. Than scrap a) Quote b) Author c) Type Of Quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766324ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first importing the required libraries\n",
    "import pandas as pd\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "\n",
    "#Now we will download the webdriver for the Web Browser. Steps for downloading are:\n",
    "#1. Check the version of your browser.\n",
    "#2. go to the link  https://chromedriver.chromium.org/downloads\n",
    "#3. Download the webdriver for your version of your browser.\n",
    "\n",
    "driver= webdriver.Chrome(r'C:\\Users\\ankush.parkhad\\Desktop\\data science\\chrome driver\\chromedriver.exe')\n",
    "\n",
    "#opening page on automated chrome browser\n",
    "driver.get(' https://www.azquotes.com/')\n",
    "\n",
    "#clicking on show hide button\n",
    "button=driver.find_element(By.XPATH,'/html/body/div[1]/div[1]/div[1]/div/div[3]/a[1]')\n",
    "button.click()\n",
    "\n",
    "#clicking on top quotes\n",
    "top=driver.find_element(By.XPATH,'/html/body/div[1]/div[1]/div[1]/div/div[3]/ul/li[5]/a')\n",
    "top.click()\n",
    "                        \n",
    "\n",
    "#creating empty lists\n",
    "quotes=[]\n",
    "author=[]\n",
    "type=[]\n",
    "\n",
    "#scrapping all quotes\n",
    "start=0\n",
    "end=10\n",
    "for page in range(start,end):\n",
    "    quote=driver.find_elements(By.XPATH,'//a[@class=\"title\"]')\n",
    "    for i in quote:\n",
    "        quotes.append(i)\n",
    "    next_button=driver.find_element(By.XPATH,'//li[@class=\"next\"]')\n",
    "    next_button.click()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ea569e66",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83a542a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "79c1bc38",
   "metadata": {},
   "source": [
    "# Q9: Write a python program to display list of respected former Prime Ministers of India(i.e. Name, Born-Dead, Term of office, Remarks) from https://www.jagranjosh.com/.\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.jagranjosh.com/\n",
    "2. Then You have to click on the GK option\n",
    "3. Then click on the List of all Prime Ministers of India\n",
    "4. Then scrap the mentioned data and make the DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "e894504b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first importing the required libraries\n",
    "import pandas as pd\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "\n",
    "#Now we will download the webdriver for the Web Browser. Steps for downloading are:\n",
    "#1. Check the version of your browser.\n",
    "#2. go to the link  https://chromedriver.chromium.org/downloads\n",
    "#3. Download the webdriver for your version of your browser.\n",
    "\n",
    "driver= webdriver.Chrome(r'C:\\Users\\ankush.parkhad\\Desktop\\data science\\chrome driver\\chromedriver.exe')\n",
    "\n",
    "driver\n",
    "\n",
    "#opening the jagranjosh page on automated chrome browser\n",
    "driver.get('https://www.jagranjosh.com/')\n",
    "\n",
    "#clicking gk option as required in the ques\n",
    "gk=driver.find_element(By.XPATH,'/html/body/div/div[1]/div/div[1]/div/div[6]/div/div[1]/header/div[3]/ul/li[9]/a')\n",
    "gk.click()\n",
    "\n",
    "#clicking List of all Prime Ministers of India\n",
    "PM=driver.find_element(By.XPATH,'/html/body/div[1]/div/div/div[2]/div/div[10]/div/div/ul/li[2]/a')\n",
    "PM.click()\n",
    "\n",
    "#Creating empty list to store the required data\n",
    "name=[]\n",
    "born_dead=[]\n",
    "term_of_office=[]\n",
    "remarks=[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e008a0e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9b76e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "91facccc",
   "metadata": {},
   "source": [
    "# Q10: Write a python program to display list of 50 Most expensive cars in the world (i.e.Car name ,Description and Price) from https://www.motor1.com/This task will be done in following steps:\n",
    "1. First get the webpage https://www.motor1.com/\n",
    "2. Then You have to click on the List option from Dropdown menu on leftside.\n",
    "3. Then click on 50 most expensive carsin the world..\n",
    "4. Then scrap the mentioned data and make the dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "0dbe4468",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first import the required libraries\n",
    "import pandas as pd\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "\n",
    "#Now we will download the webdriver for the Web Browser. Steps for downloading are:\n",
    "#1. Check the version of your browser.\n",
    "#2. go to the link  https://chromedriver.chromium.org/downloads\n",
    "#3. Download the webdriver for your version of your browser.\n",
    "\n",
    "driver=webdriver.Chrome(r'C:\\Users\\ankush.parkhad\\Desktop\\data science\\chrome driver\\chromedriver.exe')\n",
    "\n",
    "#opening the motor1 page automated chrome browser\n",
    "driver.get('https://www.motor1.com/')\n",
    "\n",
    "#clicking the dropdown menu\n",
    "drop_down=driver.find_element(By.CLASS_NAME,\"m1-hamburger-button\")\n",
    "drop_down.click()\n",
    "\n",
    "#clicking the features button to get the list option\n",
    "features=driver.find_elements(By.CLASS_NAME, \"dropdown-toggle\")[2]\n",
    "features.click()   \n",
    "\n",
    "driver.find_element(By.LINK_TEXT,'LISTS').click()\n",
    "\n",
    "#clicking the 50 most expensive cars in the world \n",
    "cars=driver.find_element(By.XPATH,'/html/body/div[3]/div[9]/div[1]/div[1]/div/div/div[2]/div/div[1]/h3/a')\n",
    "cars.click()\n",
    "\n",
    "#creating empty lists to dump the data\n",
    "car_name=[]\n",
    "car_price=[]\n",
    "car_description=[]\n",
    "\n",
    "#scrapping the names of the car\n",
    "car=driver.find_elements(By.XPATH,'//h3[@class=\"subheader\"]')\n",
    "for i in car:\n",
    "    car_name.append(i.text)\n",
    "\n",
    "#scrapping the prices of cars\n",
    "price=driver.find_elements(By.XPATH,'/html/body/div[3]/div[7]/div[2]/div[1]/div[2]/div[1]/p[4]/strong')\n",
    "for i in price:\n",
    "    car_price.append(i.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
