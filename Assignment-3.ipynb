{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "04194016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\ankush.parkhad\\anaconda3\\lib\\site-packages (4.7.2)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\ankush.parkhad\\anaconda3\\lib\\site-packages (from selenium) (0.22.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\ankush.parkhad\\anaconda3\\lib\\site-packages (from selenium) (0.9.2)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\ankush.parkhad\\anaconda3\\lib\\site-packages (from selenium) (2021.10.8)\n",
      "Requirement already satisfied: urllib3[socks]~=1.26 in c:\\users\\ankush.parkhad\\anaconda3\\lib\\site-packages (from selenium) (1.26.9)\n",
      "Requirement already satisfied: sniffio in c:\\users\\ankush.parkhad\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\ankush.parkhad\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (21.4.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in c:\\users\\ankush.parkhad\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.0.4)\n",
      "Requirement already satisfied: async-generator>=1.9 in c:\\users\\ankush.parkhad\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.10)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\ankush.parkhad\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\ankush.parkhad\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.0)\n",
      "Requirement already satisfied: idna in c:\\users\\ankush.parkhad\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.3)\n",
      "Requirement already satisfied: outcome in c:\\users\\ankush.parkhad\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\ankush.parkhad\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\ankush.parkhad\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\ankush.parkhad\\anaconda3\\lib\\site-packages (from urllib3[socks]~=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\ankush.parkhad\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0764cd",
   "metadata": {},
   "source": [
    "# Q1. 1. Write a python program which searches all the product under a particular product from www.amazon.in.  The product to be searched will be taken as input from user. For e.g. If user input is ‘guitar’. Then search for guitars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ec281a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter your product: watches\n"
     ]
    }
   ],
   "source": [
    "#importing required libraries\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "\n",
    "#connecting to the web driver\n",
    "driver=webdriver.Chrome(r'C:\\Users\\ankush.parkhad\\Desktop\\data science\\chrome driver\\chromedriver.exe')\n",
    "\n",
    "#opening the amazon page on automated chrome browser\n",
    "driver.get('https://www.amazon.in/')\n",
    "driver.maximize_window()\n",
    "time.sleep(5)\n",
    "\n",
    "prod=driver.find_element(By.ID,'twotabsearchtextbox')\n",
    "prod.click()\n",
    "user=input(\"Please enter your product: \")\n",
    "prod.send_keys(user)\n",
    "\n",
    "search=driver.find_element(By.ID,'nav-search-submit-button')\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499d0a17",
   "metadata": {},
   "source": [
    "# Q2. 2. In the above question, now scrape the following details of each product listed in first 3 pages of your search results and save it in a data frame and csv. In case if any product has less than 3 pages in search results then scrape all the products available under that product name. Details to be scraped are: \"Brand Name\", \"Name of the Product\", \"Price\", \"Return/Exchange\", \"Expected Delivery\", \"Availability\" and  “Product URL”. In case, if any of the details are missing for any of the product then replace it by “-“.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d36e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating empty lists\n",
    "brand=[]\n",
    "name_of_prod=[]\n",
    "price=[]\n",
    "exc=[]\n",
    "exp_del=[]\n",
    "avail=[]\n",
    "url=[]\n",
    "\n",
    "#scrapping brand names\n",
    "start=0\n",
    "end=2\n",
    "for page in range(start,end):\n",
    "    brands=driver.find_elements(By.XPATH,'//span[@class=\"a-size-base-plus a-color-base\"]')\n",
    "    for i in brands:\n",
    "        brand.append(i)\n",
    "    next_button=driver.find_element(By.XPATH,'/html/body/div[1]/div[2]/div[1]/div[1]/div/span[1]/div[1]/div[69]/div/div/span/a[3]')\n",
    "    next_button.click()\n",
    "\n",
    "#scrapping product name\n",
    "start=0\n",
    "end=2\n",
    "for page in range(start, end):\n",
    "    name=driver.find_elements(By.XPATH,'//h2[@class=\"a-size-mini a-spacing-none a-color-base s-line-clamp-2\"]')\n",
    "    for i in name:\n",
    "        name_of_prod.append(i.text)\n",
    "    next_button=driver.find_element(By.XPATH,'/html/body/div[1]/div[2]/div[1]/div[1]/div/span[1]/div[1]/div[69]/div/div/span/a[3]')\n",
    "    next_button.click()\n",
    "    \n",
    "\n",
    "#scrapping product price\n",
    "start=0\n",
    "end=2\n",
    "for page in range(start, end):\n",
    "    price=driver.find_elements(By.XPATH,'//span[@class=\"a-price-whole\"]')\n",
    "    for i in price:\n",
    "        price.append(i.text)\n",
    "    next_button=driver.find_element(By.XPATH,'/html/body/div[1]/div[2]/div[1]/div[1]/div/span[1]/div[1]/div[69]/div/div/span/a[3]')\n",
    "    next_button.click()\n",
    "        \n",
    "\n",
    "#scrapping return/exchange\n",
    "start=0\n",
    "end=2\n",
    "for page in range(start, end):\n",
    "    exc.append(\"-\")\n",
    "    \n",
    "\n",
    "#scrapping expected delivery\n",
    "start=0\n",
    "end=2\n",
    "for page in range(start, end):\n",
    "    deli=driver.find_elements(By.XPATH,'//span[@class=\"a-color-base a-text-bold\"]')\n",
    "    for i in deli:\n",
    "        exp_del.append(i.text)\n",
    "    next_button=driver.find_element(By.XPATH,'/html/body/div[1]/div[2]/div[1]/div[1]/div/span[1]/div[1]/div[69]/div/div/span/a[3]')\n",
    "    next_button.click()\n",
    "    \n",
    "\n",
    "#scrapping availability\n",
    "start=0\n",
    "end=2\n",
    "for page in range(start, end):\n",
    "    avail.append(\"-\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da5f34d",
   "metadata": {},
   "source": [
    "# 3. Write a python program to access the search bar and search button on images.google.com and scrape 10 images each for keywords ‘fruits’, ‘cars’ and ‘Machine Learning’, ‘Guitar’, ‘Cakes’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "490b985a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing required libraries\n",
    "import pandas as pd\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time\n",
    "\n",
    "#connecting to web browser\n",
    "driver=webdriver.Chrome(r'C:\\Users\\ankush.parkhad\\Desktop\\data science\\chrome driver\\chromedriver.exe')\n",
    "\n",
    "#oepning images.google.com on automated chrome browser\n",
    "driver.get('https://images.google.com/')\n",
    "driver.maximize_window()\n",
    "time.sleep(5)\n",
    "\n",
    "#accessing the search bar\n",
    "bar=driver.find_element(By.CLASS_NAME,'gLFyf')\n",
    "bar.send_keys('fruits')\n",
    "\n",
    "#clicking the search button\n",
    "search=driver.find_element(By.CLASS_NAME,'zgAlFc')\n",
    "search.click()\n",
    "\n",
    "for i in range(20):\n",
    "    driver.execute_script(\"window.scrollBy(0,500)\")\n",
    "    \n",
    "fruit_images=driver.find_elements(By.XPATH,'//img[@class=\"rg_i Q4LuWd\"]')\n",
    "\n",
    "image_urls=[]\n",
    "\n",
    "for i in fruit_images:\n",
    "    source=i.get_attribute('src')\n",
    "    if source is not None:\n",
    "        if(source[0:4]=='http'):\n",
    "            image_urls.append(source)\n",
    "            \n",
    "for i in range(len(image_urls)):\n",
    "    if i > 10:\n",
    "        break\n",
    "        response=requests.get(image_urls[i])\n",
    "        file=open(r\"C:\\Users\\ankush.parkhad\\Desktop\\data science\\images\"+str(i)+\".jpg\", \"wb\")\n",
    "        file.write(response.content)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5845b7",
   "metadata": {},
   "source": [
    "# Q5. Write a program to scrap geospatial coordinates (latitude, longitude) of a city searched on google maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa67ae25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing required libraries\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "#connecting to web driver\n",
    "driver=webdriver.Chrome(r'C:\\Users\\ankush.parkhad\\Desktop\\data science\\chrome driver\\chromedriver.exe')\n",
    "\n",
    "#opening google maps on automated chrome browser\n",
    "driver.get('https://www.google.com/maps/')\n",
    "driver.maximize_window()\n",
    "\n",
    "#searching a city\n",
    "city=driver.find_element(By.XPATH,'//input[@class=\"searchboxinput xiQnY\"]')\n",
    "city.send_keys('Delhi')\n",
    "\n",
    "\n",
    "#clicking search button\n",
    "search=driver.find_element(By.CLASS_NAME,'pzfvzf')\n",
    "search.click()\n",
    "\n",
    "\n",
    "url_string=driver.current_url\n",
    "lat_lng=re.findall(r'@(.*)data', url_string)\n",
    "print(lat_lng)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e811235",
   "metadata": {},
   "source": [
    "# Q7. Write a program to scrap all the available details of best gaming laptops from digit.in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "cd401ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time\n",
    "\n",
    "#connecting to web driver\n",
    "driver=webdriver.Chrome(r'C:\\Users\\ankush.parkhad\\Desktop\\data science\\chrome driver\\chromedriver.exe')\n",
    "\n",
    "#opening digit.in on automated chrome browser\n",
    "driver.get('https://www.digit.in/')\n",
    "driver.maximize_window()\n",
    "\n",
    "#clicking on best gaming laptops\n",
    "element=driver.find_element(By.XPATH,'/html/body/div[3]/div/div[2]/div[2]/div[4]/ul/li[9]/a')\n",
    "element.click()\n",
    "\n",
    "\n",
    "#creating empty lists\n",
    "name=[]\n",
    "processor=[]\n",
    "display=[]\n",
    "OS=[]\n",
    "memory=[]\n",
    "graphics_processor=[]\n",
    "body=[]\n",
    "\n",
    "\n",
    "#scrapping name of all laptops\n",
    "na=driver.find_elements(By.TAG_NAME,'h3')\n",
    "for i in na:\n",
    "    names=i.text\n",
    "    name.append(names)\n",
    "\n",
    "\n",
    "tables=driver.find_elements(By.TAG_NAME.'table')\n",
    "for i in tables:\n",
    "    trs=i.find_elements(By.TAG_NAME,'tr')\n",
    "    for tds in trs:\n",
    "        td3=tds.find_elements(By.TAG_NAME,'td')[2] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d034d043",
   "metadata": {},
   "source": [
    "# Q8.  Write a python program to scrape the details for all billionaires from www.forbes.com. Details to be scrapped: “Rank”, “Name”, “Net worth”, “Age”, “Citizenship”, “Source”, “Industry”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501c998a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time\n",
    "\n",
    "#connecting to web driver\n",
    "driver=webdriver.Chrome(r'C:\\Users\\ankush.parkhad\\Desktop\\data science\\chrome driver\\chromedriver.exe')\n",
    "\n",
    "#opening forbes.com on automated chrome browser\n",
    "driver.get('https://www.forbes.com/?sh=44ac2b552254')\n",
    "driver.maximize_window()\n",
    "time.sleep(5)\n",
    "\n",
    "#clicking on dropdown menu\n",
    "element=driver.find_element(By.CLASS_NAME,'_69hVhdY4')\n",
    "element.click()\n",
    "\n",
    "#clicking on billionaires\n",
    "element=driver.find_element(By.CLASS_NAME,'mpBfVZz3')\n",
    "element.click()\n",
    "\n",
    "#clicking on world billionaires\n",
    "element=driver.find_element(By.XPATH,'/html/body/div[1]/header/nav/div[1]/div/div/div[2]/ul/li[1]/div[2]/div[3]/ul/li[1]/a')\n",
    "element.click()\n",
    "\n",
    "#creating empty list\n",
    "rank=[]\n",
    "name=[]\n",
    "net_worth=[]\n",
    "age=[]\n",
    "citizenship=[]\n",
    "source=[]\n",
    "industry=[]\n",
    "\n",
    "#scrapping ranks\n",
    "start=0\n",
    "end=200\n",
    "for i in range(start, end):\n",
    "    ranks=driver.find_elements(By.XPATH,'//div[@class=\"rank\"]')\n",
    "    for i in ranks:\n",
    "        rank.append(i)\n",
    "    next_button=driver.find_element(By.CLASS_NAME,'bubble-arrow')\n",
    "   \n",
    "  \n",
    "\n",
    "next_button.click()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e621a4a7",
   "metadata": {},
   "source": [
    "# Q10. Write a python program to scrape a data for all available Hostels from https://www.hostelworld.com/ in “London” location. You have to scrape hostel name, distance from city centre, ratings, total reviews,  overall reviews, privates from price, dorms from price, facilities and property description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "13787402",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time\n",
    "\n",
    "#connecting to web driver\n",
    "driver=webdriver.Chrome(r'C:\\Users\\ankush.parkhad\\Desktop\\data science\\chrome driver\\chromedriver.exe')\n",
    "\n",
    "#opening hostelworld.com on automated chrome browser\n",
    "driver.get('https://www.hostelworld.com/')\n",
    "driver.maximize_window()\n",
    "time.sleep(5)\n",
    "\n",
    "#sending keys to 'where to go' search bar\n",
    "element=driver.find_element(By.CLASS_NAME,'search-input')\n",
    "element.send_keys(\"London\")\n",
    "\n",
    "#clicking on lets go button\n",
    "search=driver.find_element(By.CLASS_NAME,'search-button')\n",
    "search.click()\n",
    "\n",
    "#creating empty lists\n",
    "name=[]\n",
    "distance=[]\n",
    "ratings=[]\n",
    "total_reviews=[]\n",
    "overall_reviews=[]\n",
    "dorms_from_price=[]\n",
    "facilities=[]\n",
    "prop_desc=[]\n",
    "\n",
    "#getting all the names\n",
    "start=0\n",
    "end=1\n",
    "for page in range(start, end):\n",
    "    naam=driver.find_elements(By.XPATH,'//h2[@class=\"title title-6\"]')\n",
    "    for i in naam:\n",
    "        name.append(i.text)\n",
    "    next_button=driver.find_element(By.XPATH,'/html/body/div[3]/div/div/div[2]/div[4]/div/div/div[33]/div[5]/i')\n",
    "    next_button.click()\n",
    "\n",
    "\n",
    "#getting distance from city\n",
    "start=0\n",
    "end=1\n",
    "for page in range(start, end):\n",
    "    dis=driver.find_elements(By.XPATH,'//span[@class=\"description\"]')\n",
    "    for i in dis:\n",
    "        distance.append(i.text)\n",
    "    next_button=driver.find_element(By.XPATH,'/html/body/div[3]/div/div/div[2]/div[4]/div/div/div[33]/div[5]/i')\n",
    "    next_button.click()\n",
    "\n",
    "\n",
    "#scrapping ratings\n",
    "start=0\n",
    "end=1\n",
    "for page in range(start,end):\n",
    "    rat=driver.find_elements(By.XPATH,'//div[@class=\"score orange big\"]')\n",
    "    for i in rat:\n",
    "        ratings.append(i.text)\n",
    "        next_button=driver.find_element(By.XPATH,'/html/body/div[3]/div/div/div[2]/div[4]/div/div/div[33]/div[5]/i')\n",
    "    next_button.click()\n",
    "\n",
    "#scrapping total reviews\n",
    "start=0\n",
    "end=1\n",
    "for page in range(start, end):\n",
    "    rev=driver.find_elements(By.XPATH,'//div[@class=\"reviews\"]')\n",
    "    for i in rev:\n",
    "        total_reviews.append(i.text)\n",
    "        next_button=driver.find_element(By.XPATH,'/html/body/div[3]/div/div/div[2]/div[4]/div/div/div[33]/div[5]/i')\n",
    "    next_button.click()\n",
    "        \n",
    "\n",
    "#scrapping overall reviews\n",
    "start=0\n",
    "end=1\n",
    "for page in range(start,end):\n",
    "    overall_reviews.append(\"-\")\n",
    "\n",
    "#scrapping dorms\n",
    "start=0\n",
    "end=1\n",
    "for page in range(start, end):\n",
    "    dorm=driver.find_elements(By.XPATH,'//div[@class=\"price title-5\"]')\n",
    "    for i in dorm:\n",
    "        dorms_from_price.append(i.text)\n",
    "        next_button=driver.find_element(By.XPATH,'/html/body/div[3]/div/div/div[2]/div[4]/div/div/div[33]/div[5]/i')\n",
    "    next_button.click()\n",
    "        \n",
    "\n",
    "#scrapping facilities\n",
    "start=0\n",
    "end=1\n",
    "for page in range(start, end):\n",
    "    fac=driver.find_elements(By.XPATH,'//div[@class=\"rating-factors prop-card-tablet rating-factors small\"]')\n",
    "    for i in fac:\n",
    "        facilities.append(i.text)\n",
    "        next_button=driver.find_element(By.XPATH,'/html/body/div[3]/div/div/div[2]/div[4]/div/div/div[33]/div[5]/i')\n",
    "    next_button.click()\n",
    "        \n",
    "\n",
    "#scrapping property description\n",
    "start=0\n",
    "end=1\n",
    "for page in range(start, end):\n",
    "    desc=driver.find_elements(By.XPATH,'//div[@class=\"keyword\"]')\n",
    "    for i in desc:\n",
    "        prop_desc.append(i.text)\n",
    "        next_button=driver.find_element(By.XPATH,'/html/body/div[3]/div/div/div[2]/div[4]/div/div/div[33]/div[5]/i')\n",
    "    next_button.click()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
