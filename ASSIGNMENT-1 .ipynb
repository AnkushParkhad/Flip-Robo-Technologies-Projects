{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d46b357e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\ankush.parkhad\\anaconda3\\lib\\site-packages (2.27.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ankush.parkhad\\anaconda3\\lib\\site-packages (from requests) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\ankush.parkhad\\anaconda3\\lib\\site-packages (from requests) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\ankush.parkhad\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ankush.parkhad\\anaconda3\\lib\\site-packages (from requests) (3.3)\n",
      "Requirement already satisfied: bs4 in c:\\users\\ankush.parkhad\\anaconda3\\lib\\site-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\ankush.parkhad\\anaconda3\\lib\\site-packages (from bs4) (4.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\ankush.parkhad\\anaconda3\\lib\\site-packages (from beautifulsoup4->bs4) (2.3.1)\n"
     ]
    }
   ],
   "source": [
    "#firstly installing the required libraries\n",
    "!pip install requests\n",
    "!pip install bs4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b077e0ed",
   "metadata": {},
   "source": [
    "# Q1) Write a python program to display all the header tags from wikipedia.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "ca53adf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the list of all the header tags :\n",
      "\n",
      "<h1 class=\"firstHeading mw-first-heading\" id=\"firstHeading\" style=\"display: none\"><span class=\"mw-page-title-main\">Main Page</span></h1>\n",
      "\n",
      "<h1><span class=\"mw-headline\" id=\"Welcome_to_Wikipedia\">Welcome to <a href=\"/wiki/Wikipedia\" title=\"Wikipedia\">Wikipedia</a></span></h1>\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-tfa-h2\"><span id=\"From_today.27s_featured_article\"></span><span class=\"mw-headline\" id=\"From_today's_featured_article\">From today's featured article</span></h2>\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-dyk-h2\"><span class=\"mw-headline\" id=\"Did_you_know_...\">Did you know ...</span></h2>\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-itn-h2\"><span class=\"mw-headline\" id=\"In_the_news\">In the news</span></h2>\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-otd-h2\"><span class=\"mw-headline\" id=\"On_this_day\">On this day</span></h2>\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-tfp-h2\"><span id=\"Today.27s_featured_picture\"></span><span class=\"mw-headline\" id=\"Today's_featured_picture\">Today's featured picture</span></h2>\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-other\"><span class=\"mw-headline\" id=\"Other_areas_of_Wikipedia\">Other areas of Wikipedia</span></h2>\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-sister\"><span id=\"Wikipedia.27s_sister_projects\"></span><span class=\"mw-headline\" id=\"Wikipedia's_sister_projects\">Wikipedia's sister projects</span></h2>\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-lang\"><span class=\"mw-headline\" id=\"Wikipedia_languages\">Wikipedia languages</span></h2>\n",
      "\n",
      "<h2>Navigation menu</h2>\n",
      "\n",
      "<h3 class=\"vector-menu-heading\" id=\"p-personal-label\">\n",
      "<span class=\"vector-menu-heading-label\">Personal tools</span>\n",
      "</h3>\n",
      "\n",
      "<h3 class=\"vector-menu-heading\" id=\"p-namespaces-label\">\n",
      "<span class=\"vector-menu-heading-label\">Namespaces</span>\n",
      "</h3>\n",
      "\n",
      "<h3 class=\"vector-menu-heading\" id=\"p-views-label\">\n",
      "<span class=\"vector-menu-heading-label\">Views</span>\n",
      "</h3>\n",
      "\n",
      "<h3 class=\"vector-menu-heading\" id=\"p-navigation-label\">\n",
      "<span class=\"vector-menu-heading-label\">Navigation</span>\n",
      "</h3>\n",
      "\n",
      "<h3 class=\"vector-menu-heading\" id=\"p-interaction-label\">\n",
      "<span class=\"vector-menu-heading-label\">Contribute</span>\n",
      "</h3>\n",
      "\n",
      "<h3 class=\"vector-menu-heading\" id=\"p-tb-label\">\n",
      "<span class=\"vector-menu-heading-label\">Tools</span>\n",
      "</h3>\n",
      "\n",
      "<h3 class=\"vector-menu-heading\" id=\"p-coll-print_export-label\">\n",
      "<span class=\"vector-menu-heading-label\">Print/export</span>\n",
      "</h3>\n",
      "\n",
      "<h3 class=\"vector-menu-heading\" id=\"p-wikibase-otherprojects-label\">\n",
      "<span class=\"vector-menu-heading-label\">In other projects</span>\n",
      "</h3>\n",
      "\n",
      "<h3 class=\"vector-menu-heading\" id=\"p-lang-label\">\n",
      "<span class=\"vector-menu-heading-label\">Languages</span>\n",
      "</h3>\n"
     ]
    }
   ],
   "source": [
    "#first we are importing all the required libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#Send get request to the webpage server to get the source code of the page\n",
    "page=requests.get('https://en.wikipedia.org/wiki/Main_Page')\n",
    "page\n",
    "\n",
    "#getting all the content of the page\n",
    "soup=BeautifulSoup(page.content, 'html.parser')\n",
    "soup.prettify()\n",
    "\n",
    "#displaying all the header tags\n",
    "headers=soup.find_all(['h1','h2','h3','h4','h5','h6'])\n",
    "print('Here is the list of all the header tags :', *headers, sep='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039620c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "422cd891",
   "metadata": {},
   "source": [
    "# Q2) Write a python program to display IMDB’s Top rated 100 movies’ data (i.e. name, rating, year of release) and make data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "c50a1ee9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Movie Names</th>\n",
       "      <th>Ratings</th>\n",
       "      <th>Year of Release</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Shawshank Redemption</td>\n",
       "      <td>9.2</td>\n",
       "      <td>(1994)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Godfather</td>\n",
       "      <td>9.2</td>\n",
       "      <td>(1972)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Dark Knight</td>\n",
       "      <td>9.0</td>\n",
       "      <td>(2008)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Godfather Part II</td>\n",
       "      <td>9.0</td>\n",
       "      <td>(1974)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12 Angry Men</td>\n",
       "      <td>9.0</td>\n",
       "      <td>(1957)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Citizen Kane</td>\n",
       "      <td>8.3</td>\n",
       "      <td>(1941)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>M - Eine Stadt sucht einen Mörder</td>\n",
       "      <td>8.3</td>\n",
       "      <td>(1931)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Lawrence of Arabia</td>\n",
       "      <td>8.3</td>\n",
       "      <td>(1962)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>North by Northwest</td>\n",
       "      <td>8.2</td>\n",
       "      <td>(1959)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Vertigo</td>\n",
       "      <td>8.2</td>\n",
       "      <td>(1958)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Movie Names Ratings Year of Release\n",
       "0                  The Shawshank Redemption     9.2          (1994)\n",
       "1                             The Godfather     9.2          (1972)\n",
       "2                           The Dark Knight     9.0          (2008)\n",
       "3                     The Godfather Part II     9.0          (1974)\n",
       "4                              12 Angry Men     9.0          (1957)\n",
       "..                                      ...     ...             ...\n",
       "95                             Citizen Kane     8.3          (1941)\n",
       "96        M - Eine Stadt sucht einen Mörder     8.3          (1931)\n",
       "97                       Lawrence of Arabia     8.3          (1962)\n",
       "98                       North by Northwest     8.2          (1959)\n",
       "99                                  Vertigo     8.2          (1958)\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 430,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing the required libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "#sending the get request to the webpage server to get the source code of the page\n",
    "page=requests.get('https://www.imdb.com/chart/top/')\n",
    "page\n",
    "\n",
    "#Here we are getting response[200], means we can scrap data from this website. \n",
    "#but if you we are getting other responses such as [501],[503] means we cannot scrap data from this website using beautifulsoup,\n",
    "# then we will scrap data using celenium\n",
    "\n",
    "\n",
    "# get all the content of the webpage\n",
    "soup=BeautifulSoup(page.content)\n",
    "soup\n",
    "\n",
    "#now creating an empty list and storing all the movies names inside it\n",
    "movies=[]\n",
    "for i in soup.find_all('td',class_=\"titleColumn\"):\n",
    "    i=i.text.split(\"(\")[0]\n",
    "    i=i.split(\".\")[1]\n",
    "    i=i.replace('\\n', \"\")\n",
    "    movies.append(i)\n",
    "    \n",
    "movies[0:100]\n",
    "\n",
    "\n",
    "#creating an empty list and storing all the movies ratings inside it\n",
    "ratings=[]\n",
    "for i in soup.find_all(\"td\",class_='ratingColumn imdbRating'):\n",
    "    i=i.get_text().split(\"(\")[0].replace('\\n','')\n",
    "    ratings.append(i)\n",
    "    \n",
    "ratings[0:100]\n",
    "\n",
    "\n",
    "#Scrapping year of release\n",
    "released_year=soup.find_all('span',class_='secondaryInfo')\n",
    "released_year\n",
    "\n",
    "#now creating an empty list and storing all the releasing years of movies inside it\n",
    "year=[]\n",
    "for i in soup.find_all('span',class_='secondaryInfo'):\n",
    "    i=i.get_text().replace('\\n',' ')\n",
    "    year.append(i)\n",
    "    \n",
    "year[0:100]\n",
    "\n",
    "\n",
    "#STORING THE SCRAPPED DATA BY CREATING A DATAFRAME USING PANDAS\n",
    "df=pd.DataFrame({'Movie Names':movies, 'Ratings':ratings, 'Year of Release':year})\n",
    "df[0:100]\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1404e83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6d38efbf",
   "metadata": {},
   "source": [
    "# 3) Write a python program to display IMDB’s Top rated 100 Indian movies’ data (i.e. name, rating, year of release) and make data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "0219d7a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Movie</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Year of Release</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Golmaal</td>\n",
       "      <td>8.5</td>\n",
       "      <td>(1979)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3 Idiots</td>\n",
       "      <td>8.4</td>\n",
       "      <td>(2009)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Taare Zameen Par</td>\n",
       "      <td>8.4</td>\n",
       "      <td>(2007)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Black Friday</td>\n",
       "      <td>8.4</td>\n",
       "      <td>(2004)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Guide</td>\n",
       "      <td>8.4</td>\n",
       "      <td>(1965)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Rangeela</td>\n",
       "      <td>7.4</td>\n",
       "      <td>(1995)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Jaane Tu</td>\n",
       "      <td>7.4</td>\n",
       "      <td>(2008)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Socha Na Tha</td>\n",
       "      <td>7.4</td>\n",
       "      <td>(2005)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Qayamat Se Qayamat Tak</td>\n",
       "      <td>7.4</td>\n",
       "      <td>(1988)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Gunda</td>\n",
       "      <td>7.4</td>\n",
       "      <td>(1998)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Movie Rating Year of Release\n",
       "0                  Golmaal    8.5          (1979)\n",
       "1                 3 Idiots    8.4          (2009)\n",
       "2         Taare Zameen Par    8.4          (2007)\n",
       "3             Black Friday    8.4          (2004)\n",
       "4                    Guide    8.4          (1965)\n",
       "..                     ...    ...             ...\n",
       "95                Rangeela    7.4          (1995)\n",
       "96                Jaane Tu    7.4          (2008)\n",
       "97            Socha Na Tha    7.4          (2005)\n",
       "98  Qayamat Se Qayamat Tak    7.4          (1988)\n",
       "99                   Gunda    7.4          (1998)\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 429,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##importing the required libraries\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#sending the get request to the webpage server to get the source code of the page\n",
    "page=requests.get('https://www.imdb.com/list/ls009997493/?sort=user_rating,desc&st_dt=&mode=detail&page=1')\n",
    "page\n",
    "\n",
    "# get all the content of the webpage\n",
    "soup=BeautifulSoup(page.content, 'html.parser')\n",
    "soup.prettify()\n",
    "\n",
    "#now creating an empty list and storing all the movies names inside it\n",
    "movie_names=[]\n",
    "\n",
    "for i in soup.find_all('h3', class_='lister-item-header'):\n",
    "    i=i.text.split(\"(\")[0]\n",
    "    i=i.split(\".\")[1]\n",
    "    i=i.replace(\"\\n\",\"\")\n",
    "    movie_names.append(i)\n",
    "    \n",
    "movie_names\n",
    "\n",
    "#creating an empty list and storing all the movies ratings inside it\n",
    "rating=[]\n",
    "for i in soup.find_all('div', class_='ipl-rating-star small'):\n",
    "    i=i.text.replace(\"\\n\",\"\")\n",
    "    rating.append(i)\n",
    "    \n",
    "rating\n",
    "\n",
    "#Scrapping year of release\n",
    "year_of_release=[]\n",
    "\n",
    "for i in soup.find_all('span', class_='lister-item-year text-muted unbold'):\n",
    "    year_of_release.append(i.text)\n",
    "    \n",
    "year_of_release\n",
    "\n",
    "#STORING THE SCRAPPED DATA BY CREATING A DATAFRAME USING PANDAS\n",
    "df=pd.DataFrame({'Movie':movie_names, 'Rating':rating, 'Year of Release':year_of_release})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e0c8d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "25f2666e",
   "metadata": {},
   "source": [
    "# Q4) Write a python program to display list of respected former presidents of India(i.e. Name , Term of office) \n",
    "from https://presidentofindia.nic.in/former-presidents.htm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "id": "3f4e893f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Term of Office</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Shri Ram Nath Kovind</td>\n",
       "      <td>25 July, 2017 to 25 July, 2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Shri Pranab Mukherjee</td>\n",
       "      <td>25 July, 2012 to 25 July, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Smt Pratibha Devisingh Patil</td>\n",
       "      <td>25 July, 2007 to 25 July, 2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DR. A.P.J. Abdul Kalam</td>\n",
       "      <td>25 July, 2002 to 25 July, 2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Shri K. R. Narayanan</td>\n",
       "      <td>25 July, 1997 to 25 July, 2002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Dr Shankar Dayal Sharma</td>\n",
       "      <td>25 July, 1992 to 25 July, 1997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Shri R Venkataraman</td>\n",
       "      <td>25 July, 1987 to 25 July, 1992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Giani Zail Singh</td>\n",
       "      <td>25 July, 1982 to 25 July, 1987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Shri Neelam Sanjiva Reddy</td>\n",
       "      <td>25 July, 1977 to 25 July, 1982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Dr. Fakhruddin Ali Ahmed</td>\n",
       "      <td>24 August, 1974 to 11 February, 1977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Shri Varahagiri Venkata Giri</td>\n",
       "      <td>3 May, 1969 to 20 July, 1969 and 24 August, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Dr. Zakir Husain</td>\n",
       "      <td>13 May, 1967 to 3 May, 1969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Dr. Sarvepalli Radhakrishnan</td>\n",
       "      <td>13 May, 1962 to 13 May, 1967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Dr. Rajendra Prasad</td>\n",
       "      <td>26 January, 1950 to 13 May, 1962</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Name  \\\n",
       "0           Shri Ram Nath Kovind   \n",
       "1          Shri Pranab Mukherjee   \n",
       "2   Smt Pratibha Devisingh Patil   \n",
       "3         DR. A.P.J. Abdul Kalam   \n",
       "4           Shri K. R. Narayanan   \n",
       "5        Dr Shankar Dayal Sharma   \n",
       "6            Shri R Venkataraman   \n",
       "7               Giani Zail Singh   \n",
       "8      Shri Neelam Sanjiva Reddy   \n",
       "9       Dr. Fakhruddin Ali Ahmed   \n",
       "10  Shri Varahagiri Venkata Giri   \n",
       "11              Dr. Zakir Husain   \n",
       "12  Dr. Sarvepalli Radhakrishnan   \n",
       "13           Dr. Rajendra Prasad   \n",
       "\n",
       "                                       Term of Office  \n",
       "0                     25 July, 2017 to 25 July, 2022   \n",
       "1                     25 July, 2012 to 25 July, 2017   \n",
       "2                     25 July, 2007 to 25 July, 2012   \n",
       "3                     25 July, 2002 to 25 July, 2007   \n",
       "4                     25 July, 1997 to 25 July, 2002   \n",
       "5                     25 July, 1992 to 25 July, 1997   \n",
       "6                     25 July, 1987 to 25 July, 1992   \n",
       "7                     25 July, 1982 to 25 July, 1987   \n",
       "8                     25 July, 1977 to 25 July, 1982   \n",
       "9                24 August, 1974 to 11 February, 1977  \n",
       "10   3 May, 1969 to 20 July, 1969 and 24 August, 1...  \n",
       "11                        13 May, 1967 to 3 May, 1969  \n",
       "12                       13 May, 1962 to 13 May, 1967  \n",
       "13                   26 January, 1950 to 13 May, 1962  "
      ]
     },
     "execution_count": 431,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing the required libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#sending the get request to the webpage server to get the source code of the page\n",
    "page=requests.get('https://presidentofindia.nic.in/former-presidents.htm')\n",
    "page\n",
    "\n",
    "# get all the content of the webpage\n",
    "soup=BeautifulSoup(page.content, 'html.parser')\n",
    "soup.prettify()\n",
    "\n",
    "#creating an empty list and storing all the President's name inside it\n",
    "Name=[]\n",
    "\n",
    "for i in soup.find_all(\"div\",class_=\"presidentListing\"):\n",
    "     Name.append(i.find(\"h3\").text.split(\" (\")[0]) \n",
    "\n",
    "Name\n",
    "\n",
    "#creating an empty list and storing all term of office dates inside it\n",
    "term_of_office=[]\n",
    "\n",
    "for i in soup.find_all('div', class_='presidentListing'):\n",
    "    i=i.text.split(':')[1]\n",
    "    i=i.split('\\n')[0]\n",
    "    term_of_office.append(i)\n",
    "    \n",
    "term_of_office\n",
    "\n",
    "#creating the dataframe\n",
    "import pandas as pd\n",
    "df=pd.DataFrame({'Name':Name, 'Term of Office':term_of_office})\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675b9661",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "de0276b6",
   "metadata": {},
   "source": [
    "# I tried a lot but didnot able to scrap the data which are given in question 5 and question 6, so please pardon me for that, i raised a lot of tickets for these questions but dont know why everytime the code is showing some error and i was not able to rectify that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f34ab03",
   "metadata": {},
   "source": [
    "# Q5) Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3936e35",
   "metadata": {},
   "source": [
    "# a) Top 10 ODI teams in men’s cricket along with the records for matches, points and rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "cfa28b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "page=requests.get('https://www.icc-cricket.com/rankings/mens/team-rankings/odi')\n",
    "page\n",
    "\n",
    "soup=BeautifulSoup(page.content, 'html.parser')\n",
    "soup.prettify()\n",
    "\n",
    "teams=[]\n",
    "\n",
    "for i in soup.find_all('span', class_='u-hide-phablet'):\n",
    "    teams.append(i.text)\n",
    "    \n",
    "teams[0:10]\n",
    "\n",
    "firstteam_match=soup.find('td', class_='rankings-block__banner--matches')\n",
    "firstteam_match.text\n",
    "\n",
    "new_list=[]\n",
    "\n",
    "for i in soup.find_all('td', class_='table-body__cell u-center-text'):\n",
    "    new_list.append(i.text)\n",
    "\n",
    "    \n",
    "new_list\n",
    "\n",
    "matches=[]\n",
    "points=[]\n",
    "\n",
    "for i in range(0,len(new_list)-1,2):\n",
    "\n",
    "    matches.append(new_list[i]) # other teams matches\n",
    "\n",
    "    points.append(new_list[i+1]) # other teams points\n",
    "    \n",
    "    \n",
    "matches[0:9]\n",
    "points[0:9]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcc0d39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "022c951a",
   "metadata": {},
   "source": [
    "# Q6) Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f950be9d",
   "metadata": {},
   "source": [
    "# a) Top 10 ODI teams in women’s cricket along with the records for matches, points and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "4e33f88c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Teams</th>\n",
       "      <th>Matches</th>\n",
       "      <th>Points</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Australia</td>\n",
       "      <td>18</td>\n",
       "      <td>3,061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>South Africa</td>\n",
       "      <td>26</td>\n",
       "      <td>3,098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>England</td>\n",
       "      <td>25</td>\n",
       "      <td>2,904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>India</td>\n",
       "      <td>27</td>\n",
       "      <td>2,820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>New Zealand</td>\n",
       "      <td>24</td>\n",
       "      <td>2,425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>West Indies</td>\n",
       "      <td>24</td>\n",
       "      <td>2,334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Bangladesh</td>\n",
       "      <td>12</td>\n",
       "      <td>932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Thailand</td>\n",
       "      <td>8</td>\n",
       "      <td>572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Pakistan</td>\n",
       "      <td>24</td>\n",
       "      <td>1,519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Sri Lanka</td>\n",
       "      <td>8</td>\n",
       "      <td>353</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Teams Matches Points\n",
       "0     Australia      18  3,061\n",
       "1  South Africa      26  3,098\n",
       "2       England      25  2,904\n",
       "3         India      27  2,820\n",
       "4   New Zealand      24  2,425\n",
       "5   West Indies      24  2,334\n",
       "6    Bangladesh      12    932\n",
       "7      Thailand       8    572\n",
       "8      Pakistan      24  1,519\n",
       "9     Sri Lanka       8    353"
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "page=requests.get('https://www.icc-cricket.com/rankings/womens/team-rankings/odi')\n",
    "page\n",
    "\n",
    "soup=BeautifulSoup(page.content, 'html.parser')\n",
    "soup.prettify()\n",
    "\n",
    "teams=[]\n",
    "\n",
    "for i in soup.find_all('span', class_='u-hide-phablet'):\n",
    "    teams.append(i.text)\n",
    "    \n",
    "teams[0:10]\n",
    "\n",
    "list1=[]\n",
    "\n",
    "topteam_match=soup.find('td', class_='rankings-block__banner--matches')\n",
    "list1.append(topteam_match.text)\n",
    "\n",
    "list1\n",
    "\n",
    "\n",
    "list2=[]\n",
    "topteam_point=soup.find('td', class_='rankings-block__banner--points')\n",
    "list2.append(topteam_point.text)\n",
    "\n",
    "list2\n",
    "\n",
    "new_list=[]\n",
    "\n",
    "for i in soup.find_all('td', class_='table-body__cell u-center-text'):\n",
    "    new_list.append(i.text)\n",
    "    \n",
    "new_list\n",
    "\n",
    "matches=[]\n",
    "points=[]\n",
    "\n",
    "for i in range(0,len(new_list)-1,2):\n",
    "\n",
    "    matches.append(new_list[i]) # other teams matches\n",
    "\n",
    "    points.append(new_list[i+1]) # other teams points\n",
    "    \n",
    "matches\n",
    "points\n",
    "\n",
    "final_matches_list= list1 + matches\n",
    "final_matches_list[0:10]\n",
    "\n",
    "final_points_list= list2 + points\n",
    "final_points_list[0:10]\n",
    "\n",
    "df=pd.DataFrame({'Teams':teams[0:10], 'Matches':final_matches_list[0:10], 'Points':final_points_list[0:10]})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237d453e",
   "metadata": {},
   "source": [
    "# b) Top 10 women’s ODI Batting players along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "id": "a3e8951b",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2523662152.py, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [428]\u001b[1;36m\u001b[0m\n\u001b[1;33m    list1=[]\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "page=requests.get('https://www.icc-cricket.com/rankings/womens/player-rankings/odi')\n",
    "page\n",
    "\n",
    "soup=BeautifulSoup(page.content, 'html.parser')\n",
    "(soup.prettify()\n",
    "\n",
    "list1=[]\n",
    "\n",
    "for i in soup.find_all('div', class_='rankings-block__banner--name'):\n",
    "    list1.append(i.text)\n",
    "    \n",
    "list1\n",
    "\n",
    "list2=[]\n",
    "\n",
    "for i in soup.find_all('td',class_='table-body__cell name'):\n",
    "    i=i.text.replace(\"\\n\",\"\")\n",
    "    list2.append(i)\n",
    "\n",
    "k=list2[0:27:1]\n",
    "k[0:9]\n",
    "\n",
    "df=pd.DataFrame({'Players':k})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d275eb7",
   "metadata": {},
   "source": [
    "# Q7) Write a python program to scrape mentioned news details from https://www.cnbc.com/world/?region=world : i) Headline, ii) Time, iii) News Link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "id": "200e51ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headline</th>\n",
       "      <th>Time</th>\n",
       "      <th>URLs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The 10 countries with the least paid vacation—...</td>\n",
       "      <td>2 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/12/04/the-10-countri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Michigan couple makes $9,000/mo teaching peopl...</td>\n",
       "      <td>2 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/12/04/michigan-coupl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>These 10 cars have the greatest potential life...</td>\n",
       "      <td>3 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/12/04/these-10-cars-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Startup backed by Tesla investor promises $300...</td>\n",
       "      <td>3 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/12/04/alef-aeronauti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Manufacturing orders from China down 40% in de...</td>\n",
       "      <td>3 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/12/04/manufacturing-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Death of the internal combustion engine — Cowe...</td>\n",
       "      <td>3 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/12/04/death-of-the-i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>These stocks are cheap heading into 2023, and ...</td>\n",
       "      <td>3 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/12/04/these-stocks-a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Who will be Disney's next CEO? Here are the to...</td>\n",
       "      <td>4 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/12/04/disney-ceo-top...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Study: exercise may increase the effectiveness...</td>\n",
       "      <td>4 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/12/04/study-exercise...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>OPEC+ agrees to stick to its existing policy o...</td>\n",
       "      <td>4 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/12/04/opec-meeting-o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Parking lots are becoming as important as cars...</td>\n",
       "      <td>December 3, 2022</td>\n",
       "      <td>https://www.cnbc.com/2022/12/03/parking-lots-b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Amazon's cloud unit faces cost-sensitive custo...</td>\n",
       "      <td>December 3, 2022</td>\n",
       "      <td>https://www.cnbc.com/2022/12/03/aws-faces-cost...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Don’t overlook this health warning on your dec...</td>\n",
       "      <td>December 3, 2022</td>\n",
       "      <td>https://www.cnbc.com/2022/12/03/dont-overlook-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>How electric air taxis could shake up the airl...</td>\n",
       "      <td>December 3, 2022</td>\n",
       "      <td>https://www.cnbc.com/2022/12/03/how-electric-a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>I raised 2 successful CEOs and a doctor. Here'...</td>\n",
       "      <td>December 3, 2022</td>\n",
       "      <td>https://www.cnbc.com/2022/12/03/i-raised-2-suc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Delta pilots would get more than 30% in pay ra...</td>\n",
       "      <td>December 3, 2022</td>\n",
       "      <td>https://www.cnbc.com/2022/12/03/delta-pilots-w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Men participate less in 401(k) plans than wome...</td>\n",
       "      <td>December 3, 2022</td>\n",
       "      <td>https://www.cnbc.com/2022/12/03/men-participat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>The best U.S. states to raise a family if you ...</td>\n",
       "      <td>December 3, 2022</td>\n",
       "      <td>https://www.cnbc.com/2022/12/03/best-states-ra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Susan Cain: This Bob Dylan-inspired phrase can...</td>\n",
       "      <td>December 3, 2022</td>\n",
       "      <td>https://www.cnbc.com/2022/12/03/bestselling-au...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>The difference between this comeback and the m...</td>\n",
       "      <td>December 3, 2022</td>\n",
       "      <td>https://www.cnbc.com/2022/12/03/the-difference...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Goldman says buy these five stocks for the lon...</td>\n",
       "      <td>December 3, 2022</td>\n",
       "      <td>https://www.cnbc.com/2022/12/03/goldman-says-b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Celsius users with crypto collateral stuck tur...</td>\n",
       "      <td>December 3, 2022</td>\n",
       "      <td>https://www.cnbc.com/2022/12/03/celsius-users-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Cramer's lightning round: Let Extreme Networks...</td>\n",
       "      <td>December 2, 2022</td>\n",
       "      <td>https://www.cnbc.com/2022/12/02/cramers-lightn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Jim Cramer says these 3 apparel stocks benefit...</td>\n",
       "      <td>December 2, 2022</td>\n",
       "      <td>https://www.cnbc.com/2022/12/02/jim-cramer-say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Cramer’s week ahead: Markets need a strong job...</td>\n",
       "      <td>December 2, 2022</td>\n",
       "      <td>https://www.cnbc.com/2022/12/02/cramers-week-a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Pro Picks: Watch all of Friday's big stock cal...</td>\n",
       "      <td>December 2, 2022</td>\n",
       "      <td>https://www.cnbc.com/2022/12/02/pro-picks-watc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>There is 'enormous opportunity' in REITs, says...</td>\n",
       "      <td>December 2, 2022</td>\n",
       "      <td>https://www.cnbc.com/2022/12/02/reits-offer-en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Biden administration will end monkeypox public...</td>\n",
       "      <td>December 2, 2022</td>\n",
       "      <td>https://www.cnbc.com/2022/12/02/biden-administ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Expect more choppiness ahead after a week of m...</td>\n",
       "      <td>December 2, 2022</td>\n",
       "      <td>https://www.cnbc.com/2022/12/02/expect-more-ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>GM, LG investing $275 million to expand Tennes...</td>\n",
       "      <td>December 2, 2022</td>\n",
       "      <td>https://www.cnbc.com/2022/12/02/gm-lg-investin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Headline              Time  \\\n",
       "0   The 10 countries with the least paid vacation—...       2 Hours Ago   \n",
       "1   Michigan couple makes $9,000/mo teaching peopl...       2 Hours Ago   \n",
       "2   These 10 cars have the greatest potential life...       3 Hours Ago   \n",
       "3   Startup backed by Tesla investor promises $300...       3 Hours Ago   \n",
       "4   Manufacturing orders from China down 40% in de...       3 Hours Ago   \n",
       "5   Death of the internal combustion engine — Cowe...       3 Hours Ago   \n",
       "6   These stocks are cheap heading into 2023, and ...       3 Hours Ago   \n",
       "7   Who will be Disney's next CEO? Here are the to...       4 Hours Ago   \n",
       "8   Study: exercise may increase the effectiveness...       4 Hours Ago   \n",
       "9   OPEC+ agrees to stick to its existing policy o...       4 Hours Ago   \n",
       "10  Parking lots are becoming as important as cars...  December 3, 2022   \n",
       "11  Amazon's cloud unit faces cost-sensitive custo...  December 3, 2022   \n",
       "12  Don’t overlook this health warning on your dec...  December 3, 2022   \n",
       "13  How electric air taxis could shake up the airl...  December 3, 2022   \n",
       "14  I raised 2 successful CEOs and a doctor. Here'...  December 3, 2022   \n",
       "15  Delta pilots would get more than 30% in pay ra...  December 3, 2022   \n",
       "16  Men participate less in 401(k) plans than wome...  December 3, 2022   \n",
       "17  The best U.S. states to raise a family if you ...  December 3, 2022   \n",
       "18  Susan Cain: This Bob Dylan-inspired phrase can...  December 3, 2022   \n",
       "19  The difference between this comeback and the m...  December 3, 2022   \n",
       "20  Goldman says buy these five stocks for the lon...  December 3, 2022   \n",
       "21  Celsius users with crypto collateral stuck tur...  December 3, 2022   \n",
       "22  Cramer's lightning round: Let Extreme Networks...  December 2, 2022   \n",
       "23  Jim Cramer says these 3 apparel stocks benefit...  December 2, 2022   \n",
       "24  Cramer’s week ahead: Markets need a strong job...  December 2, 2022   \n",
       "25  Pro Picks: Watch all of Friday's big stock cal...  December 2, 2022   \n",
       "26  There is 'enormous opportunity' in REITs, says...  December 2, 2022   \n",
       "27  Biden administration will end monkeypox public...  December 2, 2022   \n",
       "28  Expect more choppiness ahead after a week of m...  December 2, 2022   \n",
       "29  GM, LG investing $275 million to expand Tennes...  December 2, 2022   \n",
       "\n",
       "                                                 URLs  \n",
       "0   https://www.cnbc.com/2022/12/04/the-10-countri...  \n",
       "1   https://www.cnbc.com/2022/12/04/michigan-coupl...  \n",
       "2   https://www.cnbc.com/2022/12/04/these-10-cars-...  \n",
       "3   https://www.cnbc.com/2022/12/04/alef-aeronauti...  \n",
       "4   https://www.cnbc.com/2022/12/04/manufacturing-...  \n",
       "5   https://www.cnbc.com/2022/12/04/death-of-the-i...  \n",
       "6   https://www.cnbc.com/2022/12/04/these-stocks-a...  \n",
       "7   https://www.cnbc.com/2022/12/04/disney-ceo-top...  \n",
       "8   https://www.cnbc.com/2022/12/04/study-exercise...  \n",
       "9   https://www.cnbc.com/2022/12/04/opec-meeting-o...  \n",
       "10  https://www.cnbc.com/2022/12/03/parking-lots-b...  \n",
       "11  https://www.cnbc.com/2022/12/03/aws-faces-cost...  \n",
       "12  https://www.cnbc.com/2022/12/03/dont-overlook-...  \n",
       "13  https://www.cnbc.com/2022/12/03/how-electric-a...  \n",
       "14  https://www.cnbc.com/2022/12/03/i-raised-2-suc...  \n",
       "15  https://www.cnbc.com/2022/12/03/delta-pilots-w...  \n",
       "16  https://www.cnbc.com/2022/12/03/men-participat...  \n",
       "17  https://www.cnbc.com/2022/12/03/best-states-ra...  \n",
       "18  https://www.cnbc.com/2022/12/03/bestselling-au...  \n",
       "19  https://www.cnbc.com/2022/12/03/the-difference...  \n",
       "20  https://www.cnbc.com/2022/12/03/goldman-says-b...  \n",
       "21  https://www.cnbc.com/2022/12/03/celsius-users-...  \n",
       "22  https://www.cnbc.com/2022/12/02/cramers-lightn...  \n",
       "23  https://www.cnbc.com/2022/12/02/jim-cramer-say...  \n",
       "24  https://www.cnbc.com/2022/12/02/cramers-week-a...  \n",
       "25  https://www.cnbc.com/2022/12/02/pro-picks-watc...  \n",
       "26  https://www.cnbc.com/2022/12/02/reits-offer-en...  \n",
       "27  https://www.cnbc.com/2022/12/02/biden-administ...  \n",
       "28  https://www.cnbc.com/2022/12/02/expect-more-ch...  \n",
       "29  https://www.cnbc.com/2022/12/02/gm-lg-investin...  "
      ]
     },
     "execution_count": 432,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing the required libraries\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#Send get request to the webpage server to get the source code of the page\n",
    "page=requests.get('https://www.cnbc.com/world/?region=world')\n",
    "page\n",
    "\n",
    "#getting all the page content\n",
    "soup=BeautifulSoup(page.content,'html.parser')\n",
    "soup.prettify()\n",
    "\n",
    "#creating an empty list and appending all the headlines in it\n",
    "headline=[]\n",
    "\n",
    "for i in soup.find_all('a', class_='LatestNews-headline'):\n",
    "    headline.append(i.text)\n",
    "    \n",
    "headline\n",
    "\n",
    "#creating an empty list and appending all the time in it\n",
    "time=[]\n",
    "\n",
    "for i in soup.find_all('time', class_='LatestNews-timestamp'):\n",
    "    time.append(i.text)\n",
    "    \n",
    "time\n",
    "\n",
    "#creating an empty list and appending all the links in it\n",
    "links=[]\n",
    "\n",
    "for i in soup.find_all('a', class_='LatestNews-headline'):\n",
    "    links.append(i.get('href'))\n",
    "    \n",
    "links\n",
    "\n",
    "#creating the dataframe\n",
    "df=pd.DataFrame({'Headline':headline, 'Time':time, 'URLs':links})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eeb4b85",
   "metadata": {},
   "source": [
    "# Q8) Write a python program to scrape the details of most downloaded articles from AI in last 90 days. https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles Scrape below mentioned details : i) Paper Title, ii) Authors, iii) Published Date, iv) Paper URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "e0e2be5e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Paper Title</th>\n",
       "      <th>Authors</th>\n",
       "      <th>Published Date</th>\n",
       "      <th>Paper URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Reward is enough</td>\n",
       "      <td>Silver, David, Singh, Satinder, Precup, Doina,...</td>\n",
       "      <td>October 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Making sense of raw input</td>\n",
       "      <td>Evans, Richard, Bošnjak, Matko and 5 more</td>\n",
       "      <td>October 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Law and logic: A review from an argumentation ...</td>\n",
       "      <td>Prakken, Henry, Sartor, Giovanni</td>\n",
       "      <td>October 2015</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Creativity and artificial intelligence</td>\n",
       "      <td>Boden, Margaret A.</td>\n",
       "      <td>August 1998</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Artificial cognition for social human–robot in...</td>\n",
       "      <td>Lemaignan, Séverin, Warnier, Mathieu and 3 more</td>\n",
       "      <td>June 2017</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Explanation in artificial intelligence: Insigh...</td>\n",
       "      <td>Miller, Tim</td>\n",
       "      <td>February 2019</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Making sense of sensory input</td>\n",
       "      <td>Evans, Richard, Hernández-Orallo, José and 3 more</td>\n",
       "      <td>April 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Conflict-based search for optimal multi-agent ...</td>\n",
       "      <td>Sharon, Guni, Stern, Roni, Felner, Ariel, Stur...</td>\n",
       "      <td>February 2015</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Between MDPs and semi-MDPs: A framework for te...</td>\n",
       "      <td>Sutton, Richard S., Precup, Doina, Singh, Sati...</td>\n",
       "      <td>August 1999</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The Hanabi challenge: A new frontier for AI re...</td>\n",
       "      <td>Bard, Nolan, Foerster, Jakob N. and 13 more</td>\n",
       "      <td>March 2020</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Evaluating XAI: A comparison of rule-based and...</td>\n",
       "      <td>van der Waa, Jasper, Nieuwburg, Elisabeth, Cre...</td>\n",
       "      <td>February 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Argumentation in artificial intelligence</td>\n",
       "      <td>Bench-Capon, T.J.M., Dunne, Paul E.</td>\n",
       "      <td>October 2007</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Algorithms for computing strategies in two-pla...</td>\n",
       "      <td>Bošanský, Branislav, Lisý, Viliam and 3 more</td>\n",
       "      <td>August 2016</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Multiple object tracking: A literature review</td>\n",
       "      <td>Luo, Wenhan, Xing, Junliang and 4 more</td>\n",
       "      <td>April 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Selection of relevant features and examples in...</td>\n",
       "      <td>Blum, Avrim L., Langley, Pat</td>\n",
       "      <td>December 1997</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>A survey of inverse reinforcement learning: Ch...</td>\n",
       "      <td>Arora, Saurabh, Doshi, Prashant</td>\n",
       "      <td>August 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Explaining individual predictions when feature...</td>\n",
       "      <td>Aas, Kjersti, Jullum, Martin, Løland, Anders</td>\n",
       "      <td>September 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>A review of possible effects of cognitive bias...</td>\n",
       "      <td>Kliegr, Tomáš, Bahník, Štěpán, Fürnkranz, Joha...</td>\n",
       "      <td>June 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Integrating social power into the decision-mak...</td>\n",
       "      <td>Pereira, Gonçalo, Prada, Rui, Santos, Pedro A.</td>\n",
       "      <td>December 2016</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>“That's (not) the output I expected!” On the r...</td>\n",
       "      <td>Riveiro, Maria, Thill, Serge</td>\n",
       "      <td>September 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Explaining black-box classifiers using post-ho...</td>\n",
       "      <td>Kenny, Eoin M., Ford, Courtney, Quinn, Molly, ...</td>\n",
       "      <td>May 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Algorithm runtime prediction: Methods &amp; evalua...</td>\n",
       "      <td>Hutter, Frank, Xu, Lin, Hoos, Holger H., Leyto...</td>\n",
       "      <td>January 2014</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Wrappers for feature subset selection</td>\n",
       "      <td>Kohavi, Ron, John, George H.</td>\n",
       "      <td>December 1997</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Commonsense visual sensemaking for autonomous ...</td>\n",
       "      <td>Suchan, Jakob, Bhatt, Mehul, Varadarajan, Srik...</td>\n",
       "      <td>October 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Quantum computation, quantum theory and AI</td>\n",
       "      <td>Ying, Mingsheng</td>\n",
       "      <td>February 2010</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Paper Title  \\\n",
       "0                                    Reward is enough   \n",
       "1                           Making sense of raw input   \n",
       "2   Law and logic: A review from an argumentation ...   \n",
       "3              Creativity and artificial intelligence   \n",
       "4   Artificial cognition for social human–robot in...   \n",
       "5   Explanation in artificial intelligence: Insigh...   \n",
       "6                       Making sense of sensory input   \n",
       "7   Conflict-based search for optimal multi-agent ...   \n",
       "8   Between MDPs and semi-MDPs: A framework for te...   \n",
       "9   The Hanabi challenge: A new frontier for AI re...   \n",
       "10  Evaluating XAI: A comparison of rule-based and...   \n",
       "11           Argumentation in artificial intelligence   \n",
       "12  Algorithms for computing strategies in two-pla...   \n",
       "13      Multiple object tracking: A literature review   \n",
       "14  Selection of relevant features and examples in...   \n",
       "15  A survey of inverse reinforcement learning: Ch...   \n",
       "16  Explaining individual predictions when feature...   \n",
       "17  A review of possible effects of cognitive bias...   \n",
       "18  Integrating social power into the decision-mak...   \n",
       "19  “That's (not) the output I expected!” On the r...   \n",
       "20  Explaining black-box classifiers using post-ho...   \n",
       "21  Algorithm runtime prediction: Methods & evalua...   \n",
       "22              Wrappers for feature subset selection   \n",
       "23  Commonsense visual sensemaking for autonomous ...   \n",
       "24         Quantum computation, quantum theory and AI   \n",
       "\n",
       "                                              Authors  Published Date  \\\n",
       "0   Silver, David, Singh, Satinder, Precup, Doina,...    October 2021   \n",
       "1           Evans, Richard, Bošnjak, Matko and 5 more    October 2021   \n",
       "2                   Prakken, Henry, Sartor, Giovanni     October 2015   \n",
       "3                                 Boden, Margaret A.      August 1998   \n",
       "4     Lemaignan, Séverin, Warnier, Mathieu and 3 more       June 2017   \n",
       "5                                        Miller, Tim    February 2019   \n",
       "6   Evans, Richard, Hernández-Orallo, José and 3 more      April 2021   \n",
       "7   Sharon, Guni, Stern, Roni, Felner, Ariel, Stur...   February 2015   \n",
       "8   Sutton, Richard S., Precup, Doina, Singh, Sati...     August 1999   \n",
       "9         Bard, Nolan, Foerster, Jakob N. and 13 more      March 2020   \n",
       "10  van der Waa, Jasper, Nieuwburg, Elisabeth, Cre...   February 2021   \n",
       "11               Bench-Capon, T.J.M., Dunne, Paul E.     October 2007   \n",
       "12       Bošanský, Branislav, Lisý, Viliam and 3 more     August 2016   \n",
       "13             Luo, Wenhan, Xing, Junliang and 4 more      April 2021   \n",
       "14                      Blum, Avrim L., Langley, Pat    December 1997   \n",
       "15                   Arora, Saurabh, Doshi, Prashant      August 2021   \n",
       "16      Aas, Kjersti, Jullum, Martin, Løland, Anders   September 2021   \n",
       "17  Kliegr, Tomáš, Bahník, Štěpán, Fürnkranz, Joha...       June 2021   \n",
       "18    Pereira, Gonçalo, Prada, Rui, Santos, Pedro A.    December 2016   \n",
       "19                      Riveiro, Maria, Thill, Serge   September 2021   \n",
       "20  Kenny, Eoin M., Ford, Courtney, Quinn, Molly, ...        May 2021   \n",
       "21  Hutter, Frank, Xu, Lin, Hoos, Holger H., Leyto...    January 2014   \n",
       "22                      Kohavi, Ron, John, George H.    December 1997   \n",
       "23  Suchan, Jakob, Bhatt, Mehul, Varadarajan, Srik...    October 2021   \n",
       "24                                   Ying, Mingsheng    February 2010   \n",
       "\n",
       "                                            Paper URL  \n",
       "0   https://www.sciencedirect.com/science/article/...  \n",
       "1   https://www.sciencedirect.com/science/article/...  \n",
       "2   https://www.sciencedirect.com/science/article/...  \n",
       "3   https://www.sciencedirect.com/science/article/...  \n",
       "4   https://www.sciencedirect.com/science/article/...  \n",
       "5   https://www.sciencedirect.com/science/article/...  \n",
       "6   https://www.sciencedirect.com/science/article/...  \n",
       "7   https://www.sciencedirect.com/science/article/...  \n",
       "8   https://www.sciencedirect.com/science/article/...  \n",
       "9   https://www.sciencedirect.com/science/article/...  \n",
       "10  https://www.sciencedirect.com/science/article/...  \n",
       "11  https://www.sciencedirect.com/science/article/...  \n",
       "12  https://www.sciencedirect.com/science/article/...  \n",
       "13  https://www.sciencedirect.com/science/article/...  \n",
       "14  https://www.sciencedirect.com/science/article/...  \n",
       "15  https://www.sciencedirect.com/science/article/...  \n",
       "16  https://www.sciencedirect.com/science/article/...  \n",
       "17  https://www.sciencedirect.com/science/article/...  \n",
       "18  https://www.sciencedirect.com/science/article/...  \n",
       "19  https://www.sciencedirect.com/science/article/...  \n",
       "20  https://www.sciencedirect.com/science/article/...  \n",
       "21  https://www.sciencedirect.com/science/article/...  \n",
       "22  https://www.sciencedirect.com/science/article/...  \n",
       "23  https://www.sciencedirect.com/science/article/...  \n",
       "24  https://www.sciencedirect.com/science/article/...  "
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing all the required libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "#sending the get request to the webpage server to get the source code of the page\n",
    "page=requests.get('https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles')\n",
    "page\n",
    "\n",
    "#Here we are getting response[200], means we can scrap data from this website. \n",
    "#but if you we are getting other responses such as [501],[503] means we cannot scrap data from this website using beautifulsoup,\n",
    "# then we will scrap data using celenium\n",
    "\n",
    "\n",
    "# get all the content of the webpage\n",
    "soup=BeautifulSoup(page.content, 'html.parser')\n",
    "soup.prettify()\n",
    "\n",
    "#creating an empty list to store the titles of all the papers and then appending all titles using for loop\n",
    "title=[]\n",
    " \n",
    "for i in soup.find_all('h2', class_='sc-1qrq3sd-1 gRGSUS sc-1nmom32-0 sc-1nmom32-1 btcbYu goSKRg'):\n",
    "    title.append(i.text)\n",
    "\n",
    "title\n",
    "\n",
    "#creating an empty list to store the Authors of all the papers and then appending all Authors using for loop\n",
    "authors=[]\n",
    "\n",
    "for i in soup.find_all('span', class_='sc-1w3fpd7-0 dnCnAO'):\n",
    "    authors.append(i.text)\n",
    "    \n",
    "authors\n",
    "\n",
    "#creating an empty list to store the Published date of all the papers and then appending all the dates using for loop\n",
    "publishdate=[]\n",
    "\n",
    "for i in soup.find_all('span', class_='sc-1thf9ly-2 dvggWt'):\n",
    "    publishdate.append(i.text)\n",
    "    \n",
    "publishdate\n",
    "\n",
    "#creating an empty list to store the URLs of all the papers and then appending all URLs using for loop\n",
    "url=[]\n",
    "\n",
    "for i in soup.find_all('a', class_='sc-5smygv-0 fIXTHm'):\n",
    "    url.append(i.get('href'))\n",
    "    \n",
    "url\n",
    "\n",
    "#Finally creating a dataframe\n",
    "df=pd.DataFrame({'Paper Title':title, 'Authors':authors, 'Published Date':publishdate,'Paper URL':url})\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bf1fa1",
   "metadata": {},
   "source": [
    "# Q9) Write a python program to scrape mentioned details from dineout.co.in :i) Restaurant name, ii) Cuisine, iii) Location, iv) Ratings, v) Image URL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "id": "a0353329",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Restaurant</th>\n",
       "      <th>Cuisine</th>\n",
       "      <th>Location</th>\n",
       "      <th>Ratings</th>\n",
       "      <th>Image URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tamasha</td>\n",
       "      <td>Continental, Asian, Italian, North Indian</td>\n",
       "      <td>Connaught Place, Central Delhi</td>\n",
       "      <td>4.2</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Local</td>\n",
       "      <td>North Indian, Asian, Continental</td>\n",
       "      <td>Scindia House,Connaught Place, Central Delhi</td>\n",
       "      <td>4</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Station Bar</td>\n",
       "      <td>Italian, Chinese, North Indian, Fast Food1 de...</td>\n",
       "      <td>F-Block,Connaught Place, Central Delhi</td>\n",
       "      <td>4</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>QBA</td>\n",
       "      <td>North Indian, Continental, Italian</td>\n",
       "      <td>Connaught Place, Central Delhi</td>\n",
       "      <td>4.3</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ministry Of Beer</td>\n",
       "      <td>North Indian, Continental, American, Asian</td>\n",
       "      <td>M-Block,Connaught Place, Central Delhi</td>\n",
       "      <td>4</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Unplugged Courtyard</td>\n",
       "      <td>North Indian, Italian, Chinese, Turkish, Cont...</td>\n",
       "      <td>Connaught Place, Central Delhi</td>\n",
       "      <td>4</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The Junkyard Cafe</td>\n",
       "      <td>North Indian, Continental, Chinese, Fast Food...</td>\n",
       "      <td>Connaught Place, Central Delhi</td>\n",
       "      <td>4.1</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The G.T. ROAD</td>\n",
       "      <td>North Indian</td>\n",
       "      <td>M-Block,Connaught Place, Central Delhi</td>\n",
       "      <td>4.3</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The Luggage Room By Sandoz</td>\n",
       "      <td>Chinese, Italian, North Indian, Continental</td>\n",
       "      <td>M-Block,Connaught Place, Central Delhi</td>\n",
       "      <td>3.9</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Ardor 2.1 Restaurant and Lounge</td>\n",
       "      <td>North Indian, Chinese, Italian, Continental</td>\n",
       "      <td>Connaught Place, Central Delhi</td>\n",
       "      <td>3.9</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Chido</td>\n",
       "      <td>North Indian, Italian, Continental, Asian, Fi...</td>\n",
       "      <td>Connaught Place, Central Delhi</td>\n",
       "      <td>4.2</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Sandoz</td>\n",
       "      <td>North Indian, Chinese, Continental</td>\n",
       "      <td>Connaught Place, Central Delhi</td>\n",
       "      <td>4</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>My Bar Headquarters</td>\n",
       "      <td>North Indian, Chinese</td>\n",
       "      <td>Connaught Place, Central Delhi</td>\n",
       "      <td>4</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Out Of The Box Courtyard</td>\n",
       "      <td>North Indian, Mediterranean, Chinese, Italian</td>\n",
       "      <td>Connaught Place, Central Delhi</td>\n",
       "      <td>4.1</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Connaught Clubhouse Microbrewery</td>\n",
       "      <td>North Indian, Continental, Asian, Chinese</td>\n",
       "      <td>Connaught Place, Central Delhi</td>\n",
       "      <td>4.3</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Openhouse Cafe</td>\n",
       "      <td>North Indian, Asian, Italian</td>\n",
       "      <td>Connaught Place, Central Delhi</td>\n",
       "      <td>4.1</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Berco's</td>\n",
       "      <td>Chinese, Thai</td>\n",
       "      <td>Connaught Place, Central Delhi</td>\n",
       "      <td>4.3</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>38 Barracks</td>\n",
       "      <td>North Indian, Chinese, Continental</td>\n",
       "      <td>M-Block,Connaught Place, Central Delhi</td>\n",
       "      <td>4.3</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Cafe Delhi Heights</td>\n",
       "      <td>Continental, North Indian, Beverages, Chinese...</td>\n",
       "      <td>Janpath, Central Delhi</td>\n",
       "      <td>4.3</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Chili's American Grill and Bar</td>\n",
       "      <td>Mexican, American, Tex Mex</td>\n",
       "      <td>M-Block,Connaught Place, Central Delhi</td>\n",
       "      <td>4.3</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Wok in the Clouds</td>\n",
       "      <td>Chinese, Thai, Continental, North Indian, Asian</td>\n",
       "      <td>Connaught Place, Central Delhi</td>\n",
       "      <td>4.3</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Restaurant  \\\n",
       "0                            Tamasha   \n",
       "1                              Local   \n",
       "2                        Station Bar   \n",
       "3                                QBA   \n",
       "4                   Ministry Of Beer   \n",
       "5                Unplugged Courtyard   \n",
       "6                  The Junkyard Cafe   \n",
       "7                      The G.T. ROAD   \n",
       "8         The Luggage Room By Sandoz   \n",
       "9    Ardor 2.1 Restaurant and Lounge   \n",
       "10                             Chido   \n",
       "11                            Sandoz   \n",
       "12               My Bar Headquarters   \n",
       "13          Out Of The Box Courtyard   \n",
       "14  Connaught Clubhouse Microbrewery   \n",
       "15                    Openhouse Cafe   \n",
       "16                           Berco's   \n",
       "17                       38 Barracks   \n",
       "18                Cafe Delhi Heights   \n",
       "19    Chili's American Grill and Bar   \n",
       "20                 Wok in the Clouds   \n",
       "\n",
       "                                              Cuisine  \\\n",
       "0           Continental, Asian, Italian, North Indian   \n",
       "1                    North Indian, Asian, Continental   \n",
       "2    Italian, Chinese, North Indian, Fast Food1 de...   \n",
       "3                  North Indian, Continental, Italian   \n",
       "4          North Indian, Continental, American, Asian   \n",
       "5    North Indian, Italian, Chinese, Turkish, Cont...   \n",
       "6    North Indian, Continental, Chinese, Fast Food...   \n",
       "7                                        North Indian   \n",
       "8         Chinese, Italian, North Indian, Continental   \n",
       "9         North Indian, Chinese, Italian, Continental   \n",
       "10   North Indian, Italian, Continental, Asian, Fi...   \n",
       "11                 North Indian, Chinese, Continental   \n",
       "12                              North Indian, Chinese   \n",
       "13      North Indian, Mediterranean, Chinese, Italian   \n",
       "14          North Indian, Continental, Asian, Chinese   \n",
       "15                       North Indian, Asian, Italian   \n",
       "16                                      Chinese, Thai   \n",
       "17                 North Indian, Chinese, Continental   \n",
       "18   Continental, North Indian, Beverages, Chinese...   \n",
       "19                         Mexican, American, Tex Mex   \n",
       "20    Chinese, Thai, Continental, North Indian, Asian   \n",
       "\n",
       "                                        Location Ratings  \\\n",
       "0                 Connaught Place, Central Delhi     4.2   \n",
       "1   Scindia House,Connaught Place, Central Delhi       4   \n",
       "2         F-Block,Connaught Place, Central Delhi       4   \n",
       "3                 Connaught Place, Central Delhi     4.3   \n",
       "4         M-Block,Connaught Place, Central Delhi       4   \n",
       "5                 Connaught Place, Central Delhi       4   \n",
       "6                 Connaught Place, Central Delhi     4.1   \n",
       "7         M-Block,Connaught Place, Central Delhi     4.3   \n",
       "8         M-Block,Connaught Place, Central Delhi     3.9   \n",
       "9                 Connaught Place, Central Delhi     3.9   \n",
       "10                Connaught Place, Central Delhi     4.2   \n",
       "11                Connaught Place, Central Delhi       4   \n",
       "12                Connaught Place, Central Delhi       4   \n",
       "13                Connaught Place, Central Delhi     4.1   \n",
       "14                Connaught Place, Central Delhi     4.3   \n",
       "15                Connaught Place, Central Delhi     4.1   \n",
       "16                Connaught Place, Central Delhi     4.3   \n",
       "17        M-Block,Connaught Place, Central Delhi     4.3   \n",
       "18                        Janpath, Central Delhi     4.3   \n",
       "19        M-Block,Connaught Place, Central Delhi     4.3   \n",
       "20                Connaught Place, Central Delhi     4.3   \n",
       "\n",
       "                                            Image URL  \n",
       "0   https://im1.dineout.co.in/images/uploads/resta...  \n",
       "1   https://im1.dineout.co.in/images/uploads/resta...  \n",
       "2   https://im1.dineout.co.in/images/uploads/resta...  \n",
       "3   https://im1.dineout.co.in/images/uploads/resta...  \n",
       "4   https://im1.dineout.co.in/images/uploads/resta...  \n",
       "5   https://im1.dineout.co.in/images/uploads/resta...  \n",
       "6   https://im1.dineout.co.in/images/uploads/resta...  \n",
       "7   https://im1.dineout.co.in/images/uploads/resta...  \n",
       "8   https://im1.dineout.co.in/images/uploads/resta...  \n",
       "9   https://im1.dineout.co.in/images/uploads/resta...  \n",
       "10  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "11  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "12  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "13  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "14  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "15  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "16  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "17  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "18  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "19  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "20  https://im1.dineout.co.in/images/uploads/resta...  "
      ]
     },
     "execution_count": 433,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing the required libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "#Send get request to the webpage server to get the source code of the page\n",
    "page=requests.get('https://www.dineout.co.in/delhi-restaurants/welcome-back')\n",
    "page\n",
    "\n",
    "#getting all the content of the page\n",
    "soup=BeautifulSoup(page.content,'html.parser')\n",
    "soup.prettify()\n",
    "\n",
    "#creating an empty list and appending all the restaurant names in it using for loop\n",
    "title=[]\n",
    "\n",
    "for i in soup.find_all('a', class_='restnt-name ellipsis'):\n",
    "    title.append(i.text)\n",
    "\n",
    "title\n",
    "\n",
    "#creating an empty list and appending all the cuisines in it using for loop\n",
    "cuisine=[]\n",
    "\n",
    "for i in soup.find_all('div', class_='detail-info'):\n",
    "    i=i.text.split(\"|\")[1]\n",
    "    cuisine.append(i)\n",
    "    \n",
    "cuisine\n",
    "\n",
    "#creating an empty list and appending all the restaurant location in it using for loop\n",
    "location=[]\n",
    "\n",
    "for i in soup.find_all('div', class_='restnt-loc ellipsis'):\n",
    "    location.append(i.text)\n",
    "    \n",
    "location\n",
    "\n",
    "#creating an empty list and appending all the restaurant ratings in it using for loop\n",
    "ratings=[]\n",
    "\n",
    "for i in soup.find_all('div', class_='restnt-rating rating-4'):\n",
    "    ratings.append(i.text)\n",
    "    \n",
    "ratings\n",
    "\n",
    "#creating an empty list and appending all the restaurant image url in it using for loop\n",
    "image_url=[]\n",
    "\n",
    "for i in soup.find_all('img', class_='no-img'):\n",
    "    image_url.append(i.get('data-src'))\n",
    "    \n",
    "image_url\n",
    "\n",
    "#finally creating the dataframe\n",
    "df=pd.DataFrame({'Restaurant':title, 'Cuisine':cuisine, 'Location':location,'Ratings':ratings, 'Image URL':image_url})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64172ac8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "719c10a6",
   "metadata": {},
   "source": [
    "# 10) Write a python program to scrape the details of top publications from Google Scholar from  https://scholar.google.com/citations?view_op=top_venues&hl=en i) Rank , ii) Publication, iii) h5-index, iv) h5-median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ac4cdb2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Publication</th>\n",
       "      <th>h5_Index</th>\n",
       "      <th>h5_Median</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.</td>\n",
       "      <td>Nature</td>\n",
       "      <td>444</td>\n",
       "      <td>667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.</td>\n",
       "      <td>The New England Journal of Medicine</td>\n",
       "      <td>432</td>\n",
       "      <td>780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.</td>\n",
       "      <td>Science</td>\n",
       "      <td>401</td>\n",
       "      <td>614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.</td>\n",
       "      <td>IEEE/CVF Conference on Computer Vision and Pat...</td>\n",
       "      <td>389</td>\n",
       "      <td>627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.</td>\n",
       "      <td>The Lancet</td>\n",
       "      <td>354</td>\n",
       "      <td>635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>96.</td>\n",
       "      <td>Journal of Business Research</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>97.</td>\n",
       "      <td>Molecular Cancer</td>\n",
       "      <td>145</td>\n",
       "      <td>209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>98.</td>\n",
       "      <td>Sensors</td>\n",
       "      <td>145</td>\n",
       "      <td>201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>99.</td>\n",
       "      <td>Nature Climate Change</td>\n",
       "      <td>144</td>\n",
       "      <td>228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>100.</td>\n",
       "      <td>IEEE Internet of Things Journal</td>\n",
       "      <td>144</td>\n",
       "      <td>212</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Rank                                        Publication h5_Index h5_Median\n",
       "0     1.                                             Nature      444       667\n",
       "1     2.                The New England Journal of Medicine      432       780\n",
       "2     3.                                            Science      401       614\n",
       "3     4.  IEEE/CVF Conference on Computer Vision and Pat...      389       627\n",
       "4     5.                                         The Lancet      354       635\n",
       "..   ...                                                ...      ...       ...\n",
       "95   96.                       Journal of Business Research      145       233\n",
       "96   97.                                   Molecular Cancer      145       209\n",
       "97   98.                                            Sensors      145       201\n",
       "98   99.                              Nature Climate Change      144       228\n",
       "99  100.                    IEEE Internet of Things Journal      144       212\n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing all the required libraries\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#Send get request to the webpage server to get the source code of the page\n",
    "page=requests.get('https://scholar.google.com/citations?view_op=top_venues&hl=en')\n",
    "page\n",
    "\n",
    "#get all the content of the page\n",
    "soup=BeautifulSoup(page.content, 'html.parser')\n",
    "soup.prettify()\n",
    "\n",
    "#creating an empty list and appending all the ranks in it using for loop\n",
    "rank=[]\n",
    "\n",
    "for i in soup.find_all('td', class_='gsc_mvt_p'):\n",
    "    rank.append(i.text)\n",
    "rank\n",
    "\n",
    "#creating an empty list and appending all thepublications in it using for loop\n",
    "Publication=[]\n",
    "\n",
    "for i in soup.find_all('td', 'gsc_mvt_t'):\n",
    "    Publication.append(i.text)\n",
    "Publication\n",
    "\n",
    "#creating an empty list and appending all the h5_index in it using for loop\n",
    "h5_index=[]\n",
    "\n",
    "for i in soup.find_all('a', class_='gs_ibl gsc_mp_anchor'):\n",
    "    h5_index.append(i.text)\n",
    "    \n",
    "h5_index\n",
    "\n",
    "##creating an empty list and appending all the h5_median in it using for loop\n",
    "h5_median=[]\n",
    "\n",
    "for i in soup.find_all('span', class_='gs_ibl gsc_mp_anchor'):\n",
    "    h5_median.append(i.text)\n",
    "    \n",
    "h5_median\n",
    "# finally creating the dataframe\n",
    "df=pd.DataFrame({'Rank':rank, 'Publication':Publication, 'h5_Index':h5_index, 'h5_Median':h5_median})\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
